{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.spaces import Dict as DictSpace\n",
    "from gymnasium.spaces import Discrete\n",
    "from pettingzoo import ParallelEnv\n",
    "from skimage.transform import resize\n",
    "\n",
    "from aij_multiagent_rl.engine import GameEngine\n",
    "\n",
    "TimestampType = Tuple[\n",
    "    Dict[str, Dict[str, np.ndarray]], Dict[str, float],\n",
    "    Dict[str, bool], Dict[str, bool], Dict[str, dict]\n",
    "]\n",
    "\n",
    "MOVES = [\"FORWARD\", \"LEFT\", \"RIGHT\", \"BACKWARD\",\n",
    "         \"PICKUP_RESOURCE\", \"PICKUP_TRASH\",\n",
    "         \"DROP_RESOURCE\", \"DROP_TRASH\", \"NOOP\"]\n",
    "\n",
    "\n",
    "class AijMultiagentEnv(ParallelEnv):\n",
    "    \"\"\"Parallel multi-agent environment for AIJ\n",
    "    Multi-Agent RL contest\n",
    "\n",
    "    Environment at the testing system will have the same\n",
    "    hyperparameter setting as below, so it is not recommended\n",
    "    to change it\n",
    "\n",
    "    Attributes:\n",
    "        grid_size: 2D square game field size\n",
    "        obs_dim: local visual observation size\n",
    "        move_step: movement step size in pixels\n",
    "        resource_price: reward given for resource processing\n",
    "        recycle_cost: cost of recycling trash\n",
    "        border_distort_range: noise range for borders distortion\n",
    "        max_edge_dev: maximum segment border shift\n",
    "        max_tries: maximum attempts for border generation\n",
    "        machine_size: machine icon size in pixels\n",
    "        machine_reach: size of machine interaction region\n",
    "        agent_size: agent icon size in pixels\n",
    "        agent_reach: agent reach when picking up items\n",
    "        resource_size: resource icon size in pixels\n",
    "        trash_size: trash icon size in pixels\n",
    "        resource_prob: probability to spawn resource at a given point\n",
    "        border_display_width: segments borders thickness\n",
    "        ecology_penalty: decrease in agent's ecology score caused by 1 trash item\n",
    "        neighbour_ecology_weight: neighbour ecology effect at the resource\n",
    "            respawn rate\n",
    "        global_ecology_weight: global ecology effect at the resource\n",
    "            respawn rate\n",
    "        init_respawn_prob: initial probability to spawn resource at a given point\n",
    "        blocked_vanish_alpha: blocked segment fogging degree\n",
    "        max_dead_segments: max number of blocked segments before global\n",
    "            termination\n",
    "    \"\"\"\n",
    "    # Hardcoded hyperparameters\n",
    "    grid_size: int = 210  # should be div by resource_size and trash_size\n",
    "    obs_dim: int = 60  # should be divisible by 5\n",
    "    move_step: int = 7\n",
    "    resource_price: int = 10\n",
    "    recycle_cost: int = 4\n",
    "    border_distort_range: Tuple[int, int] = (-1, 2)\n",
    "    max_edge_dev: float = 0.1\n",
    "    max_tries: int = 25\n",
    "    machine_size: int = 9\n",
    "    machine_reach: int = 9\n",
    "    agent_size: int = 9\n",
    "    agent_reach: int = 9\n",
    "    resource_size: int = 7\n",
    "    trash_size: int = 5\n",
    "    resource_prob: float = 0.075\n",
    "    border_display_width: int = 2\n",
    "    ecology_penalty: int = 20\n",
    "    neighbour_ecology_weight: float = 0.2\n",
    "    global_ecology_weight: float = 0.3\n",
    "    init_respawn_prob: float = 0.015\n",
    "    blocked_vanish_alpha: float = 0.25\n",
    "    max_dead_segments: int = 4\n",
    "\n",
    "    # To enable rendering\n",
    "    metadata = {\"render_modes\": ['rgb_array'],\n",
    "                \"name\": \"aij_multiagent_env\"}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_cycles: Optional[int] = 1000,\n",
    "        state_size: Optional[int] = 110,\n",
    "        render_mode: Optional[str] = 'rgb_array',\n",
    "    ):\n",
    "        \"\"\"Multi-agent RL Environment\n",
    "\n",
    "        Multi-agent RL Environment for AIJ Contest 2024\n",
    "\n",
    "        Args:\n",
    "            max_cycles: maximum simulation length in time steps\n",
    "            state_size: display state size for rendering\n",
    "            render_mode: render mode\n",
    "\n",
    "        Valid Action Space:\n",
    "            0: move forward by `move_step` pixels if possible\n",
    "            1: move left by `move_step` pixels if possible\n",
    "            2: move right by `move_step` pixels if possible\n",
    "            3: move backward by `move_step` pixels if possible\n",
    "            4: pickup resource (if closer than `agent_reach` pixels)\n",
    "            5: pickup trash (if closer than `agent_reach` pixels)\n",
    "            6: throw resource (put into machine if closer than `machine_reach`)\n",
    "            7: throw trash (put into recycler if closer than `machine_reach`)\n",
    "            8: noop\n",
    "        \"\"\"\n",
    "        self.engine = None\n",
    "        self.rng = None\n",
    "        self.ecology_scores = None\n",
    "        self.num_moves = None\n",
    "        self.current_state = None\n",
    "        self.seed = None\n",
    "        self.render_mode = render_mode\n",
    "        self.max_cycles = max_cycles\n",
    "        self.state_size = state_size\n",
    "        self.action_meanings = MOVES\n",
    "        self.possible_agents = [f'agent_{i}' for i in range(8)]\n",
    "        self.neighbours_mapping = {\n",
    "            'agent_0': ['agent_1', 'agent_3'],\n",
    "            'agent_1': ['agent_0', 'agent_2'],\n",
    "            'agent_2': ['agent_1', 'agent_4'],\n",
    "            'agent_3': ['agent_0', 'agent_5'],\n",
    "            'agent_4': ['agent_2', 'agent_7'],\n",
    "            'agent_5': ['agent_3', 'agent_6'],\n",
    "            'agent_6': ['agent_5', 'agent_7'],\n",
    "            'agent_7': ['agent_4', 'agent_6'],\n",
    "        }\n",
    "        self.agents = self.possible_agents.copy()\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _get_engine(\n",
    "        cls, seed: Optional[int] = None\n",
    "    ) -> Tuple[GameEngine, np.random.Generator, int]:\n",
    "        \"\"\"Get engine\n",
    "\n",
    "        Get game engine for simulation round\n",
    "\n",
    "        Args:\n",
    "            seed: random seed for simulation\n",
    "\n",
    "        Returns:\n",
    "            Tuple[GameEngine, np.random.Generator, int]: tuple of:\n",
    "                - GameEngine instance\n",
    "                - numpy random number generator\n",
    "                - seed for logging\n",
    "        \"\"\"\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0, int(1e6), 1).item()\n",
    "        engine = GameEngine(\n",
    "            seed=seed,\n",
    "            grid_size=cls.grid_size,\n",
    "            obs_dim=cls.obs_dim,\n",
    "            move_step=cls.move_step,\n",
    "            resource_price=cls.resource_price,\n",
    "            recycle_cost=cls.recycle_cost,\n",
    "            border_distort_range=cls.border_distort_range,\n",
    "            max_edge_dev=cls.max_edge_dev,\n",
    "            max_tries=cls.max_tries,\n",
    "            machine_size=cls.machine_size,\n",
    "            machine_reach=cls.machine_reach,\n",
    "            agent_size=cls.agent_size,\n",
    "            agent_reach=cls.agent_reach,\n",
    "            resource_size=cls.resource_size,\n",
    "            trash_size=cls.trash_size,\n",
    "            resource_prob=cls.resource_prob,\n",
    "            border_display_width=cls.border_display_width,\n",
    "            blocked_vanish_alpha=cls.blocked_vanish_alpha\n",
    "        )\n",
    "        rng = np.random.default_rng(seed + 1)\n",
    "        return engine, rng, seed\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent) -> DictSpace:\n",
    "        \"\"\"Get observation space\n",
    "\n",
    "        Get environment observation space\n",
    "\n",
    "        Args:\n",
    "            agent: agent to return observation space for\n",
    "\n",
    "        Returns:\n",
    "            DictSpace: specification of composite observation\n",
    "        \"\"\"\n",
    "        d = self.engine.obs_dim\n",
    "        gs, diag = self.engine.grid_size, self.engine.diag\n",
    "        image_space = Box(0, 255, (d, d, 3), np.uint8, seed=self.rng)\n",
    "        low = np.array([0., 0., 0., 0., -1., -1., 0.])\n",
    "        high = np.array([np.inf, 1., 1., diag / gs, 1., 1., 1.])\n",
    "        proprio_space = Box(\n",
    "            low=low, high=high, shape=(7,), dtype=np.float32, seed=self.rng)\n",
    "        return DictSpace({'image': image_space, 'proprio': proprio_space})\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent) -> Discrete:\n",
    "        \"\"\"Get action space\n",
    "\n",
    "        Get environment action space\n",
    "\n",
    "        Args:\n",
    "            agent: agent to return action space for\n",
    "\n",
    "        Returns:\n",
    "            Discrete: specification of discrete action space\n",
    "        \"\"\"\n",
    "        return Discrete(len(self.action_meanings), seed=self.rng)\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, options: Optional[Any] = None\n",
    "    ) -> Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict]]:\n",
    "        \"\"\"Reset environment\n",
    "\n",
    "        Reset environment to its initial state\n",
    "\n",
    "        Args:\n",
    "            seed: numpy random seed\n",
    "            options: any other additional options\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict]]:\n",
    "                tuple which contains:\n",
    "                    - composite observations for each agent\n",
    "                    - infos for each agent\n",
    "        \"\"\"\n",
    "        self.num_moves = 0\n",
    "        self.agents = self.possible_agents.copy()\n",
    "        self.engine, self.rng, self.seed = self._get_engine(seed=seed)\n",
    "        self.ecology_scores = {a: 100 for a in self.agents}\n",
    "        # Render next observations\n",
    "        state = self.engine.get_state()\n",
    "        self.current_state = state\n",
    "        # Local perspective state\n",
    "        local_agents_map = self.engine.agents_map(local_mode=True)\n",
    "        local_state = state * np.logical_not(\n",
    "            local_agents_map > 0)\n",
    "        local_state = local_state + local_agents_map\n",
    "        # Render local observations\n",
    "        state_pad = self.engine.pad_state(local_state)\n",
    "        obs = {a: self.engine.local_obs(\n",
    "            state_pad=state_pad, agent_id=a) for a in self.agents}\n",
    "        # Log information\n",
    "        infos = {a: {\n",
    "            'ecology_score': self.ecology_scores[a],\n",
    "            'num_trash': self.engine.trash_by_segment(agent_id=a),\n",
    "            'num_resource': self.engine.resource_by_segment(agent_id=a),\n",
    "            'dead_ecology': self.engine.blocked[a]\n",
    "        } for a in self.agents}\n",
    "        return obs, infos\n",
    "\n",
    "    def _make_action(self, agent_id: str, action_id: int) -> None:\n",
    "        \"\"\"Make action\n",
    "\n",
    "        Execute action in the environment for a given agent\n",
    "\n",
    "        Args:\n",
    "             agent_id: agent ID in form `agent_{i}`\n",
    "             action_id: action integer id from valid actions set\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if action ID is outside valid action set\n",
    "        \"\"\"\n",
    "        if 4 > action_id >= 0:\n",
    "            self.engine.move(agent_id=agent_id, action_id=action_id)\n",
    "        elif action_id == 4:\n",
    "            self.engine.pickup(agent_id=agent_id, type='resource')\n",
    "        elif action_id == 5:\n",
    "            self.engine.pickup(agent_id=agent_id, type='trash')\n",
    "        elif action_id == 6:\n",
    "            self.engine.drop_resource(agent_id=agent_id)\n",
    "        elif action_id == 7:\n",
    "            self.engine.drop_trash(agent_id=agent_id)\n",
    "        elif action_id == 8:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Invalid action: {action_id} for agent: {agent_id}')\n",
    "\n",
    "    def _update_ecology_scores(self) -> Dict[str, int]:\n",
    "        \"\"\"Update ecology scores\n",
    "\n",
    "        Update ecology scores given the most recent trash\n",
    "        distribution information\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, int]: current number of trash per segment\n",
    "        \"\"\"\n",
    "        new_ec_scores = {}\n",
    "        trash_by_agent = {}\n",
    "        for a, s in self.ecology_scores.items():\n",
    "            n_trash = self.engine.trash_by_segment(agent_id=a)\n",
    "            trash_by_agent[a] = n_trash\n",
    "            ec_score = max(0, 100 - n_trash * self.ecology_penalty)\n",
    "            new_ec_scores[a] = ec_score * int(not self.engine.blocked[a])\n",
    "        self.ecology_scores = new_ec_scores\n",
    "        return trash_by_agent\n",
    "\n",
    "    def _get_resource_respawn_probs(self) -> Dict[str, float]:\n",
    "        \"\"\"Get resource respawn probs\n",
    "\n",
    "        Get resource respawn probability given current trash\n",
    "        distribution\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, int]: respawn probs by agent ID\n",
    "        \"\"\"\n",
    "        resp_probs = {}\n",
    "        gs = np.mean(list(self.ecology_scores.values())).item()\n",
    "        for a, s in self.ecology_scores.items():\n",
    "            n1, n2 = self.neighbours_mapping[a]\n",
    "            ns1, ns2 = self.ecology_scores[n1], self.ecology_scores[n2]\n",
    "            mean_ns = (ns1 + ns2) / 2\n",
    "            nw, p = self.neighbour_ecology_weight, self.init_respawn_prob\n",
    "            gw = self.global_ecology_weight\n",
    "            lw = max(0., 1 - nw - gw)\n",
    "            r = ((lw * s + gw * gs + nw * mean_ns) / 100)\n",
    "            resp_probs[a] = (r ** 2.15) * p * int(not self.engine.blocked[a])\n",
    "        return resp_probs\n",
    "\n",
    "    def _get_terminations(self) -> Dict[str, bool]:\n",
    "        \"\"\"Get terminations\n",
    "\n",
    "        Get global termination and impose blocks according to\n",
    "        local ecology scores\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, bool]: global termination by agent ID\n",
    "                (all True or all False)\n",
    "        \"\"\"\n",
    "        global_termination = False\n",
    "        for a, s in self.ecology_scores.items():\n",
    "            terminated = s == 0\n",
    "            if terminated:\n",
    "                self.engine.add_block(agent_id=a)\n",
    "        n_blocked = sum(list(self.engine.blocked.values()))\n",
    "        if n_blocked > self.max_dead_segments:\n",
    "            global_termination = True\n",
    "        return {a: global_termination for a in self.possible_agents}\n",
    "\n",
    "    def state(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Get global state\n",
    "\n",
    "        Get global state for CTDE multi-agent RL paradigm.\n",
    "        Note! That method won't be called at testing system\n",
    "        and may be used only for training agents.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, np.ndarray]: global state with following key-value\n",
    "                pairs:\n",
    "                    - 'image': global visual state, shape:\n",
    "                        (self.state_size, self.state_size, 3)\n",
    "                    - 'wealth': array with agents wealth, shape: (8,)\n",
    "                    - 'has_resource': array with binary flag, indicating that\n",
    "                        resource is in inventory, shape: (8,)\n",
    "                    - 'has_resource': array with binary flag, indicating that\n",
    "                        trash is in inventory, shape: (8,)\n",
    "        \"\"\"\n",
    "        state = {}\n",
    "        image = self.current_state.copy()\n",
    "        image = resize(image, (self.state_size, self.state_size))\n",
    "        state['image'] = np.round(image * 255, 0).astype(np.uint8)\n",
    "        state['wealth'] = np.array(\n",
    "            [self.engine.agents_state[a]['wealth']\n",
    "             for a in self.possible_agents])\n",
    "        state['has_resource'] = np.array(\n",
    "            [self.engine.agents_state[a]['inventory']['resource']\n",
    "             for a in self.possible_agents]).astype(int)\n",
    "        state['has_trash'] = np.array(\n",
    "            [self.engine.agents_state[a]['inventory']['trash']\n",
    "             for a in self.possible_agents]).astype(int)\n",
    "        return state\n",
    "\n",
    "    def render(self) -> np.ndarray:\n",
    "        \"\"\"Render global state\n",
    "\n",
    "        Render global state as numpy image array\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: global visual state, shape:\n",
    "                (self.state_size, self.state_size, 3)\n",
    "        \"\"\"\n",
    "        return self.state()['image']\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close all rendering windows\"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, actions: Dict[str, int]) -> TimestampType:\n",
    "        \"\"\"Perform simulation step\n",
    "\n",
    "        Perform simulation step in parallel multi-agent RL style.\n",
    "        In case of conflicting actions (for example two agents\n",
    "        picking up the same resource, priorities are assigned\n",
    "        randomly). Note: both terminations and truncations occur\n",
    "        simultaneously for all agents participating in the\n",
    "        simulation.\n",
    "\n",
    "        Args:\n",
    "            actions: dictionary with action IDs for each agent\n",
    "        Returns:\n",
    "            TimestampType: environment time stamp as a tuple of:\n",
    "                - Dict[str, Dict[str, np.ndarray]]: composite observations\n",
    "                    for each agent\n",
    "                - Dict[str, float]: rewards for each agent\n",
    "                - Dict[str, bool]: terminations for each agent\n",
    "                - Dict[str, bool]: truncations for each agent\n",
    "                - Dict[str, dict]: infos for each agent\"\"\"\n",
    "        self.num_moves += 1\n",
    "        # Cache money to calculate rewards\n",
    "        old_w = {a: self.engine.agents_state[a]['wealth'] for a in self.agents}\n",
    "        # Apply actions in random order (to reconcile possible conflicts)\n",
    "        agents = self.agents.copy()\n",
    "        self.rng.shuffle(agents)\n",
    "        for agent in agents:\n",
    "            self._make_action(agent_id=agent, action_id=actions[agent])\n",
    "        # Get rewards from updated wealth\n",
    "        rewards = {a: self.engine.agents_state[a]['wealth'] - old_w[a]\n",
    "                   for a in self.agents}\n",
    "        # Apply rules to update ecology scores\n",
    "        trash_by_agent = self._update_ecology_scores()\n",
    "        # Terminate or truncate for those neeeded\n",
    "        terminations = self._get_terminations()\n",
    "        truncation = self.num_moves >= self.max_cycles\n",
    "        truncations = {a: truncation for a in self.agents}\n",
    "        # Respawn resources according to respawn probabilities\n",
    "        for a, p in self._get_resource_respawn_probs().items():\n",
    "            u = self.rng.uniform(low=0, high=1)\n",
    "            if u < p:\n",
    "                self.engine.sample_resource(agent_id=a)\n",
    "        # Render next observations\n",
    "        state = self.engine.get_state()\n",
    "        self.current_state = state\n",
    "        # Local perspective state\n",
    "        local_agents_map = self.engine.agents_map(local_mode=True)\n",
    "        local_state = state * np.logical_not(\n",
    "            local_agents_map > 0)\n",
    "        local_state = local_state + local_agents_map\n",
    "        # Render local observations\n",
    "        state_pad = self.engine.pad_state(local_state)\n",
    "        observations = {a: self.engine.local_obs(\n",
    "            state_pad=state_pad, agent_id=a) for a in self.agents}\n",
    "        # Log information\n",
    "        infos = {a: {\n",
    "            'ecology_score': self.ecology_scores[a],\n",
    "            'num_trash': trash_by_agent[a],\n",
    "            'num_resource': self.engine.resource_by_segment(agent_id=a),\n",
    "            'dead_ecology': self.engine.blocked[a]\n",
    "        } for a in self.agents}\n",
    "        # Delete truncated and terminated Agents\n",
    "        if truncation or any(terminations.values()):\n",
    "            self.agents = []\n",
    "        return observations, rewards, terminations, truncations, infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: distrax in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (0.1.5)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from distrax) (2.0.2)\n",
      "Requirement already satisfied: jaxlib>=0.1.67 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from distrax) (0.4.30)\n",
      "Requirement already satisfied: absl-py>=0.9.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from distrax) (2.2.2)\n",
      "Requirement already satisfied: chex>=0.1.8 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from distrax) (0.1.89)\n",
      "Requirement already satisfied: jax>=0.1.55 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from distrax) (0.4.30)\n",
      "Requirement already satisfied: tensorflow-probability>=0.15.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from distrax) (0.25.0)\n",
      "Requirement already satisfied: typing_extensions>=4.2.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from chex>=0.1.8->distrax) (4.13.2)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from chex>=0.1.8->distrax) (1.0.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from jax>=0.1.55->distrax) (0.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from jax>=0.1.55->distrax) (8.7.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from jax>=0.1.55->distrax) (1.13.1)\n",
      "Requirement already satisfied: opt-einsum in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from jax>=0.1.55->distrax) (3.4.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.6->jax>=0.1.55->distrax) (3.21.0)\n",
      "Requirement already satisfied: gast>=0.3.2 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from tensorflow-probability>=0.15.0->distrax) (0.6.0)\n",
      "Requirement already satisfied: dm-tree in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from tensorflow-probability>=0.15.0->distrax) (0.1.8)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from tensorflow-probability>=0.15.0->distrax) (3.1.1)\n",
      "Requirement already satisfied: decorator in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from tensorflow-probability>=0.15.0->distrax) (5.2.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-probability>=0.15.0->distrax) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install distrax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax import struct\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any, Tuple, Union, Dict\n",
    "import functools \n",
    "import distrax\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "  \"\"\"Class defining the convolution model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    dtype = jnp.float32\n",
    "    x = x.astype(dtype) / 255.\n",
    "    x = nn.Conv(features=32, kernel_size=(8, 8), strides=(4, 4), name='conv1',\n",
    "                dtype=dtype)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Conv(features=64, kernel_size=(4, 4), strides=(2, 2), name='conv2',\n",
    "                dtype=dtype)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3), strides=(1, 1), name='conv3',\n",
    "                dtype=dtype)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = x.reshape(x.shape[0],x.shape[1], -1) #flatten\n",
    "    return x\n",
    "\n",
    "class ScannedRNN(nn.Module):\n",
    "    @functools.partial(\n",
    "        nn.scan,\n",
    "        variable_broadcast=\"params\",\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        split_rngs={\"params\": False},\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        \"\"\"Applies the module.\"\"\"\n",
    "        rnn_state = carry\n",
    "        ins, resets = x\n",
    "        rnn_state = jnp.where(\n",
    "            resets[:, np.newaxis],\n",
    "            self.initialize_carry(*rnn_state.shape),\n",
    "            rnn_state,\n",
    "        )\n",
    "        new_rnn_state, y = nn.GRUCell(features=ins.shape[1])(rnn_state, ins)\n",
    "        return new_rnn_state, y\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(batch_size, hidden_size):\n",
    "        # Use a dummy key since the default state init fn is just zeros.\n",
    "        cell = nn.GRUCell(features=hidden_size)\n",
    "        return cell.initialize_carry(jax.random.PRNGKey(0), (batch_size, hidden_size))\n",
    "\n",
    "class ActorRNN(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    config: dict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        obs, dones = x\n",
    "        image, vector = obs\n",
    "        processed_image = ConvEncoder()(image)\n",
    "        processed_vector = nn.Dense(self.config[\"FC_DIM_SIZE\"], kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(vector)\n",
    "        full_obs = jax.lax.concatenate([processed_image, processed_vector], 2)\n",
    "        embedding = nn.Dense(\n",
    "            self.config[\"FC_DIM_SIZE\"], kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(full_obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "\n",
    "        actor_mean = nn.Dense(self.config[\"GRU_HIDDEN_DIM\"], kernel_init=orthogonal(2), bias_init=constant(0.0))(\n",
    "            embedding\n",
    "        )\n",
    "        actor_mean = nn.relu(actor_mean)\n",
    "        action_logits = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "\n",
    "        pi = distrax.Categorical(logits=action_logits)\n",
    "\n",
    "        return hidden, pi\n",
    "\n",
    "class CriticRNN(nn.Module):\n",
    "    config: Dict\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, x):\n",
    "        world_state, dones = x\n",
    "        image, vector = world_state\n",
    "        processed_image = ConvEncoder()(image)\n",
    "        processed_vector = nn.Dense(self.config[\"FC_DIM_SIZE\"], kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(vector)\n",
    "        full_obs = jax.lax.concatenate([processed_image, processed_vector], 2)\n",
    "        embedding = nn.Dense(\n",
    "            self.config[\"FC_DIM_SIZE\"], kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(full_obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "        \n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "        \n",
    "        critic = nn.Dense(self.config[\"GRU_HIDDEN_DIM\"], kernel_init=orthogonal(2), bias_init=constant(0.0))(\n",
    "            embedding\n",
    "        )\n",
    "        critic = nn.relu(critic)\n",
    "        critic = nn.Dense(self.config['NUM_CRITIC_OUTS'], kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "        \n",
    "        return hidden, jnp.squeeze(critic, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from typing import NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "\n",
    "NUM_AGENTS = 8\n",
    "AGENT_KEYS = [f'agent_{i}' for i in range(NUM_AGENTS)]\n",
    "AGENT_OBS_KEYS = ['proprio','image']\n",
    "CENTR_OBS_KEYS = ['wealth','has_resource','has_trash']\n",
    "AGENT_AREA_KEYS = ['ecology_score','num_trash','num_resource','dead_ecology']\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    global_done: jnp.ndarray\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    world_state: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "\n",
    "def batchify(x: dict, num_actors):\n",
    "    x = jnp.stack([x[a] for a in AGENT_KEYS])\n",
    "    return x.reshape((num_actors, -1))\n",
    "\n",
    "def unbatchify(x: jnp.ndarray, num_envs, num_actors):\n",
    "    x = x.reshape((num_actors, num_envs, -1))\n",
    "    return {a: x[i] for i, a in enumerate(AGENT_KEYS)}\n",
    "\n",
    "def linear_schedule(count, config):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "def create_train_state(module: nn.Module, module_params: jnp.ndarray, config: dict[str, Any]) -> TrainState:\n",
    "    tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "    train_state = TrainState.create(\n",
    "            apply_fn=module.apply,\n",
    "            params=module_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: flax in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (0.8.5)\n",
      "Requirement already satisfied: jax>=0.4.27 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from flax) (0.4.30)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from flax) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from flax) (4.13.2)\n",
      "Requirement already satisfied: tensorstore in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from flax) (0.1.69)\n",
      "Requirement already satisfied: rich>=11.1 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from flax) (14.0.0)\n",
      "Requirement already satisfied: msgpack in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from flax) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from flax) (2.0.2)\n",
      "Requirement already satisfied: optax in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from flax) (0.2.4)\n",
      "Requirement already satisfied: orbax-checkpoint in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from flax) (0.6.4)\n",
      "Requirement already satisfied: opt-einsum in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from jax>=0.4.27->flax) (3.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from jax>=0.4.27->flax) (8.7.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from jax>=0.4.27->flax) (1.13.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from jax>=0.4.27->flax) (0.5.1)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from jax>=0.4.27->flax) (0.4.30)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.6->jax>=0.4.27->flax) (3.21.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from rich>=11.1->flax) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from rich>=11.1->flax) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from optax->flax) (2.2.2)\n",
      "Requirement already satisfied: etils[epy] in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from optax->flax) (1.5.2)\n",
      "Requirement already satisfied: chex>=0.1.87 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from optax->flax) (0.1.89)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from chex>=0.1.87->optax->flax) (1.0.0)\n",
      "Requirement already satisfied: nest_asyncio in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Requirement already satisfied: protobuf in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from orbax-checkpoint->flax) (6.30.2)\n",
      "Requirement already satisfied: humanize in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from orbax-checkpoint->flax) (4.12.3)\n",
      "Requirement already satisfied: importlib_resources in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from etils[epy]->optax->flax) (6.5.2)\n",
      "Requirement already satisfied: fsspec in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from etils[epy]->optax->flax) (2025.3.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "!pip3 install flax\n",
    "import flax.linen as nn\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AijMultiagentEnv()\n",
    "initial_agents_state, initial_area_state = env.reset()\n",
    "initial_world_state = env.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mappo_config.yaml\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"NUM_ACTORS\"] = NUM_AGENTS * config[\"NUM_ENVS\"]\n",
    "config[\"NUM_UPDATES\"] = (\n",
    "    config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    ")\n",
    "config[\"MINIBATCH_SIZE\"] = (\n",
    "    config[\"NUM_ACTORS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    ")\n",
    "config[\"CLIP_EPS\"] = config[\"CLIP_EPS\"] / env.num_agents if config[\"SCALE_CLIP_EPS\"] else config[\"CLIP_EPS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(config[\"SEED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, _rng_actor, _rng_critic = jax.random.split(rng, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_init_image = jnp.zeros((1, config[\"NUM_ENVS\"], *initial_agents_state[AGENT_KEYS[0]]['image'].shape))\n",
    "actor_init_proprio = jnp.zeros((1, config[\"NUM_ENVS\"], *initial_agents_state[AGENT_KEYS[0]]['proprio'].shape))\n",
    "actor_init_obs = (actor_init_image, actor_init_proprio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_init_x = (\n",
    "            actor_init_obs,\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"])),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 60, 60, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_init_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorRNN(env.action_space(AGENT_KEYS[0]).n, config=config)\n",
    "ac_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], config[\"GRU_HIDDEN_DIM\"])\n",
    "actor_network_params = actor.init(_rng_actor, ac_init_hstate, actor_init_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_image = jnp.zeros((1, config[\"NUM_ENVS\"], *env.state()['image'].shape))\n",
    "critic_additional_obs = jnp.concatenate([env.state()[_] for _ in CENTR_OBS_KEYS], dtype = jnp.float32)\n",
    "area_obs = jnp.array([list(initial_area_state[_].values()) for _ in AGENT_KEYS],dtype = jnp.float32).reshape(-1)\n",
    "critic_feats = jnp.concatenate([critic_additional_obs,area_obs]).reshape(1,config[\"NUM_ENVS\"],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_init_x = (\n",
    "            (critic_image,critic_feats),\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"])),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = CriticRNN(config=config)\n",
    "cr_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], config[\"GRU_HIDDEN_DIM\"])\n",
    "critic_network_params = critic.init(_rng_critic, cr_init_hstate, cr_init_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_train_state = create_train_state(actor, actor_network_params, config)\n",
    "critic_train_state = create_train_state(critic, critic_network_params, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, _rng = jax.random.split(rng)\n",
    "reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "obsv, env_state = env.reset(seed=reset_rng[0][0].item())\n",
    "ac_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ACTORS\"], config[\"GRU_HIDDEN_DIM\"])\n",
    "cr_init_hstate = ScannedRNN.initialize_carry(config[\"NUM_ACTORS\"], config[\"GRU_HIDDEN_DIM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
