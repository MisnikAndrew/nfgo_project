{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d470cd-bafb-4deb-afd6-308c6ca5bfa1",
   "metadata": {},
   "source": [
    "# Бейзлайн для задачи AIJ Multi-Agent AI\n",
    "\n",
    "Данный ноутбук содержит реализацию [VDN](https://arxiv.org/abs/1706.05296) - кооперативного \n",
    "мульти-агентного алгоритма обучения с подкреплением. VDN основан на предпосылке\n",
    "о линейном разложении общей награды агентов, таким образом, общая награда\n",
    "всех агентов представлена в виде суммы индивидуальных наград.\n",
    "Несмотря на то, что данная предпосылка ограничивает класс обучаемых стратегий\n",
    "только кооперативными вариантами, VDN все же является хорошим бейзлайном для \n",
    "многих задач мульти-агентного обучения с подкреплением.\n",
    "\n",
    "Данный бейзлайн позволяет получить целевую метрику (Mean Focal Score) около\n",
    "42 при ее сабмите в тестовую систему (Случайная политика, для сравнения, \n",
    "получает ~4).\n",
    "\n",
    "Главным результатом работы ноутбука будет создание директории `submission_vdn`, которую \n",
    "необходимо запаковать в .zip архив и отправить в тестирующую систему.\n",
    "\n",
    "Мы рекомендуем запускать тесты на своих решениях, прежде чем отправлять их в систему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18da7c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.24.0-cp39-cp39-macosx_12_0_arm64.whl (13.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.4 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from scikit-image) (25.0)\n",
      "Collecting imageio>=2.33\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "\u001b[K     |████████████████████████████████| 315 kB 52.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tifffile>=2022.8.12\n",
      "  Downloading tifffile-2024.8.30-py3-none-any.whl (227 kB)\n",
      "\u001b[K     |████████████████████████████████| 227 kB 50.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=9.1\n",
      "  Downloading pillow-11.2.1-cp39-cp39-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 65.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lazy-loader>=0.4\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting numpy>=1.23\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 20.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.8\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 21.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.9\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.3 MB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pillow, numpy, tifffile, scipy, networkx, lazy-loader, imageio, scikit-image\n",
      "Successfully installed imageio-2.37.0 lazy-loader-0.4 networkx-3.2.1 numpy-2.0.2 pillow-11.2.1 scikit-image-0.24.0 scipy-1.13.1 tifffile-2024.8.30\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# engine\n",
    "\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "# !pip3 install scipy\n",
    "!pip3 install scikit-image\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL.Image import Image as ImageType\n",
    "from scipy.ndimage import uniform_filter\n",
    "from skimage import draw\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "COLOR_MAPS = {\n",
    "    'red': np.array([255, 1, 1]),\n",
    "    'blue': np.array([1, 1, 255]),\n",
    "    'purple': np.array([128, 1, 128]),\n",
    "    'pink': np.array([255, 200, 255]),\n",
    "    'yellow': np.array([255, 255, 100]),\n",
    "    'orange': np.array([235, 155, 1]),\n",
    "    'gray': np.array([128, 128, 128]),\n",
    "    'turquoise': np.array([1, 219, 255]),\n",
    "    'domestic': np.array([255, 1, 1]),\n",
    "    'foreign': np.array([51, 51, 51]),\n",
    "}\n",
    "\n",
    "\n",
    "def create_borders_candidate(\n",
    "    grid_size: int,\n",
    "    border_distort_range: Tuple[int, int],\n",
    "    max_edge_dev: float,\n",
    "    rng: np.random.Generator\n",
    ") -> Tuple[np.ndarray, np.ndarray, bool]:\n",
    "    \"\"\"Create candidate border split\n",
    "\n",
    "    Create candidate for agents' segments randomised split\n",
    "\n",
    "    Args:\n",
    "        grid_size: 2D square game field size\n",
    "        border_distort_range: noise range for borders distortion\n",
    "        max_edge_dev: maximum segment border shift\n",
    "        rng: numpy random number generator\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, bool]: tuple of\n",
    "            (horizontal borders, vertical borders, validity flag)\n",
    "    \"\"\"\n",
    "    h_borders = np.zeros((grid_size, grid_size))\n",
    "    v_borders = np.zeros((grid_size, grid_size))\n",
    "    border_steps = np.linspace(0, grid_size, 4)[1:-1].astype(int)\n",
    "    low, high = border_distort_range\n",
    "    valid = True\n",
    "\n",
    "    # horizontal borders\n",
    "    h_inds = []\n",
    "    for bs in border_steps:\n",
    "        h_bord = rng.integers(low=low, high=high, size=(grid_size,))\n",
    "        h_bord = np.cumsum(h_bord)\n",
    "        h_bord += bs\n",
    "        h_inds.append(h_bord)\n",
    "        h_borders[h_bord, range(h_borders.shape[1])] = 1\n",
    "        # check for drift\n",
    "        if (np.abs(h_bord[-1] - bs) / grid_size) > max_edge_dev:\n",
    "            valid = False\n",
    "\n",
    "    # Border overlap\n",
    "    if max(h_inds[0]) > min(h_inds[1]):\n",
    "        valid = False\n",
    "\n",
    "    # vertical borders\n",
    "    v_inds = []\n",
    "    for bs in border_steps:\n",
    "        v_bord = rng.integers(low=low, high=high, size=(grid_size,))\n",
    "        v_bord = np.cumsum(v_bord)\n",
    "        v_bord += bs\n",
    "        v_inds.append(v_bord)\n",
    "        v_borders[range(v_borders.shape[0]), v_bord] = 1\n",
    "        # check for drift\n",
    "        if (np.abs(v_bord[-1] - bs) / grid_size) > max_edge_dev:\n",
    "            valid = False\n",
    "\n",
    "    # Border overlap\n",
    "    if max(v_inds[0]) > min(v_inds[1]):\n",
    "        valid = False\n",
    "\n",
    "    return h_borders, v_borders, valid\n",
    "\n",
    "\n",
    "def create_borders(\n",
    "    grid_size: int,\n",
    "    border_distort_range: Tuple[int, int],\n",
    "    max_edge_dev: float,\n",
    "    rng: np.random.Generator,\n",
    "    max_tries: Optional[int] = 10\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create valid borders\n",
    "\n",
    "    Create valid borders with multiple randomized attempts\n",
    "    until success\n",
    "\n",
    "    Args:\n",
    "        grid_size: 2D square game field size\n",
    "        border_distort_range: noise range for borders distortion\n",
    "        max_edge_dev: maximum segment border shift\n",
    "        rng: numpy random number generator\n",
    "        max_tries: maximum attempts for border generation\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: tuple of\n",
    "            (horizontal borders, vertical borders)\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "    n_tries = 0\n",
    "    h_borders, v_borders = None, None\n",
    "    while not valid:\n",
    "        h_borders, v_borders, valid = create_borders_candidate(\n",
    "            grid_size=grid_size,\n",
    "            border_distort_range=border_distort_range,\n",
    "            max_edge_dev=max_edge_dev,\n",
    "            rng=rng\n",
    "        )\n",
    "        n_tries += 1\n",
    "        if n_tries > max_tries:\n",
    "            raise AssertionError(\n",
    "                f'Exceeded max number of generation attempts: {max_tries}'\n",
    "            )\n",
    "    return h_borders, v_borders\n",
    "\n",
    "\n",
    "def get_segment_map(\n",
    "    h_borders: np.ndarray,\n",
    "    v_borders: np.ndarray\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Create segment map\n",
    "\n",
    "    Create valid segment split for simulation\n",
    "\n",
    "    Args:\n",
    "        h_borders: np.ndarray of horizontal borders\n",
    "        v_borders: np.ndarray of vertical borders\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, np.ndarray]: dictionary of binary masks\n",
    "            for each segment on the map\n",
    "    \"\"\"\n",
    "    h_csum = np.cumsum(h_borders, axis=0)\n",
    "    v_csum = (np.cumsum(v_borders, axis=1) + 1) * 10\n",
    "    raw_segment_map = (h_csum + v_csum).astype(int)\n",
    "    seg_ids = sorted(np.unique(raw_segment_map))\n",
    "    agent_id = 0\n",
    "    segment_map = {}\n",
    "    for s in seg_ids:\n",
    "        if s != 21:  # non water case\n",
    "            segment_map[f'agent_{agent_id}'] = (\n",
    "                    raw_segment_map == s).astype(int)\n",
    "            agent_id += 1\n",
    "        else:  # water case\n",
    "            segment_map['water'] = (raw_segment_map == s).astype(int)\n",
    "    return segment_map\n",
    "\n",
    "\n",
    "def get_machines_candidate(\n",
    "    s_map: np.ndarray,\n",
    "    distance: int,\n",
    "    machine_size: int,\n",
    "    rng: np.random.Generator\n",
    ") -> Tuple[np.ndarray, ...]:\n",
    "    \"\"\"Create candidate placement for machines\n",
    "\n",
    "    Create candidate placement for machines for given\n",
    "    individual agent segment\n",
    "\n",
    "    Args:\n",
    "        s_map: binary segment map for a given agent\n",
    "        distance: initial distance between fabricator and recycler\n",
    "        machine_size: machine icon size in pixels\n",
    "        rng:  numpy random number generator\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, ...]: tuple of numpy arrays which has\n",
    "            - fabricator center location\n",
    "            - recycler center location\n",
    "            - fabricator binary map\n",
    "            - recycler binary map\n",
    "            - segment center location\n",
    "    \"\"\"\n",
    "    center = (np\n",
    "              .argwhere(s_map == 1)\n",
    "              .mean(axis=0)\n",
    "              .round(0)\n",
    "              .astype(int))\n",
    "    rad = rng.uniform(0, 2 * np.pi)\n",
    "    h_comp = distance * np.cos(rad)\n",
    "    v_comp = distance * np.sin(rad)\n",
    "    machine_loc = np.array(\n",
    "        [center[0] + h_comp, center[1] + v_comp]\n",
    "    ).round(0).astype(int)\n",
    "    recycler_loc = np.array(\n",
    "        [center[0] - h_comp, center[1] - v_comp]\n",
    "    ).round(0).astype(int)\n",
    "    # machines loc\n",
    "    s = machine_size // 2\n",
    "    ms = machine_size\n",
    "    machine_map = np.pad(\n",
    "        np.zeros_like(s_map), ms, constant_values=0\n",
    "    ).astype(int)\n",
    "    recycler_map = np.pad(\n",
    "        np.zeros_like(s_map), ms, constant_values=0\n",
    "    ).astype(int)\n",
    "    machine_map[(machine_loc[0] - s + ms):(machine_loc[0] + s + ms + 1),\n",
    "                (machine_loc[1] - s + ms):(machine_loc[1] + s + ms + 1)] = 1\n",
    "    recycler_map[(recycler_loc[0] - s + ms):(recycler_loc[0] + s + ms + 1),\n",
    "                 (recycler_loc[1] - s + ms):(recycler_loc[1] + s + ms + 1)] = 1\n",
    "    return machine_loc, recycler_loc, machine_map, recycler_map, center\n",
    "\n",
    "\n",
    "def validate_locs(\n",
    "    s_map: np.ndarray,\n",
    "    machine_size: int,\n",
    "    machine_map: np.ndarray,\n",
    "    recycler_map: np.ndarray\n",
    ") -> bool:\n",
    "    \"\"\"Validate machines placement\n",
    "\n",
    "    Validate machines placement\n",
    "\n",
    "    Args:\n",
    "        s_map: binary segment map for a given agent\n",
    "        machine_size: machine icon size in pixels\n",
    "        machine_map: fabricator binary map\n",
    "        recycler_map: recycler binary map\n",
    "\n",
    "    Returns:\n",
    "        bool: machines placement validity\n",
    "    \"\"\"\n",
    "    s_map_pad = np.pad(\n",
    "        s_map, machine_size, constant_values=0\n",
    "    ).astype(int)\n",
    "    negative_map = np.logical_not(s_map_pad)\n",
    "    valid = True\n",
    "    total_map = np.logical_or(machine_map, recycler_map)\n",
    "    if np.logical_and(negative_map, total_map).sum() > 0:\n",
    "        # check for borders violation\n",
    "        valid = False\n",
    "    if np.logical_and(machine_map, recycler_map).sum() > 0:\n",
    "        # check for no intersection between\n",
    "        valid = False\n",
    "    return valid\n",
    "\n",
    "\n",
    "def get_machines_loc(\n",
    "    s_map: np.ndarray,\n",
    "    machine_size: int,\n",
    "    rng: np.random.Generator\n",
    ") -> Tuple[np.ndarray, ...]:\n",
    "    \"\"\"Create placement for machines\n",
    "\n",
    "    Create placement for machines for a given individual agent\n",
    "    segment\n",
    "\n",
    "    Args:\n",
    "        s_map: binary segment map for a given agent\n",
    "        machine_size: machine icon size in pixels\n",
    "        rng:  numpy random number generator\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, ...]: tuple of numpy arrays which has\n",
    "            - fabricator center location\n",
    "            - recycler center location\n",
    "            - fabricator binary map\n",
    "            - recycler binary map\n",
    "            - segment center location\n",
    "    \"\"\"\n",
    "    distance = s_map.shape[0] // 6\n",
    "    valid = False\n",
    "    while not valid and distance > 0:\n",
    "        locs = get_machines_candidate(\n",
    "            s_map=s_map, machine_size=machine_size,\n",
    "            distance=distance, rng=rng\n",
    "        )\n",
    "        machine_loc, recycler_loc, machine_map, recycler_map, center = locs\n",
    "        valid = validate_locs(\n",
    "            s_map=s_map, machine_size=machine_size,\n",
    "            machine_map=machine_map, recycler_map=recycler_map\n",
    "        )\n",
    "        distance -= 1\n",
    "    if not valid:\n",
    "        raise AssertionError(\n",
    "            'Valid machines locations not found'\n",
    "        )\n",
    "    # get rid of padding for maps\n",
    "    p = machine_size\n",
    "    sum_pad = np.logical_or(machine_map, recycler_map).sum()\n",
    "    machine_map = machine_map[p:-p, p:-p]\n",
    "    recycler_map = recycler_map[p:-p, p:-p]\n",
    "    sum_crop = np.logical_or(machine_map, recycler_map).sum()\n",
    "    assert sum_pad == sum_crop, 'crop is invalid'\n",
    "    return machine_loc, recycler_loc, machine_map, recycler_map, center\n",
    "\n",
    "\n",
    "def get_all_machines_loc(\n",
    "    segment_map: Dict[str, np.ndarray],\n",
    "    machine_size: int,\n",
    "    rng: np.random.Generator\n",
    ") -> Tuple[Dict[str, Any], ...]:\n",
    "    \"\"\"Create placement for machines\n",
    "\n",
    "    Create placement for machines for all agents segments\n",
    "\n",
    "    Args:\n",
    "        segment_map: dictionary with binary segment maps for each agent\n",
    "        machine_size: machine icon size in pixels\n",
    "        rng:  numpy random number generator\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Any], ...]: dictionaries with information\n",
    "            about fabricators, recyclers and segment centers placement\n",
    "    \"\"\"\n",
    "    machines, recyclers, centers = {}, {}, {}\n",
    "    for seg, s_map in segment_map.items():\n",
    "        if seg != 'water':\n",
    "            ml, rl, mm, rm, c = get_machines_loc(\n",
    "                s_map=s_map, machine_size=machine_size,\n",
    "                rng=rng\n",
    "            )\n",
    "            machines[seg] = {}\n",
    "            recyclers[seg] = {}\n",
    "            centers[seg] = c\n",
    "            machines[seg]['center'] = ml\n",
    "            machines[seg]['map'] = mm\n",
    "            recyclers[seg]['center'] = rl\n",
    "            recyclers[seg]['map'] = rm\n",
    "    return machines, recyclers, centers\n",
    "\n",
    "\n",
    "def get_non_resource_regions(\n",
    "    segment_map: Dict[str, np.ndarray],\n",
    "    machine_size: int,\n",
    "    machines: Dict[str, Dict[str, np.ndarray]],\n",
    "    recyclers: Dict[str, Dict[str, np.ndarray]],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Get non resource regions\n",
    "\n",
    "    Get binary map for regions where resources can\n",
    "    not be located\n",
    "\n",
    "    Args:\n",
    "        segment_map: dictionary with binary segment maps for each agent\n",
    "        machine_size: machine icon size in pixels\n",
    "        machines: placement information for fabricators\n",
    "        recyclers: placement information for recyclers\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: binary map for regions where resources can\n",
    "            not be located\n",
    "    \"\"\"\n",
    "    non_resource = segment_map['water'].copy()\n",
    "    for a in machines.keys():\n",
    "        mm = machines[a]['map']\n",
    "        rm = recyclers[a]['map']\n",
    "        am = np.logical_or(mm, rm)\n",
    "        non_resource = np.logical_or(non_resource, am)\n",
    "    non_resource = uniform_filter(\n",
    "        non_resource.astype(float),\n",
    "        size=int(machine_size),\n",
    "        mode='constant'\n",
    "    )\n",
    "    return (non_resource > 1e-5).astype(np.uint8)\n",
    "\n",
    "\n",
    "def get_region_grid(\n",
    "    region: np.ndarray,\n",
    "    cell_size: int,\n",
    "    agg_fn: Optional = np.max\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Aggregate 2d array\n",
    "\n",
    "    Aggregate 2d array with a given agg_func\n",
    "\n",
    "    Args:\n",
    "        region: region numerical representation\n",
    "        cell_size: 2D aggregation window size\n",
    "        agg_fn: function to aggregate with\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: aggregated region representation\n",
    "    \"\"\"\n",
    "    region_grid = block_reduce(\n",
    "        region,\n",
    "        (cell_size, cell_size),\n",
    "        agg_fn\n",
    "    )\n",
    "    return region_grid\n",
    "\n",
    "\n",
    "def get_segment_map_grid(\n",
    "    segment_map: Dict[str, np.ndarray],\n",
    "    cell_size: int,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Aggregate segment map\n",
    "\n",
    "    Aggregate binary segment map\n",
    "\n",
    "    Args:\n",
    "        segment_map: dictionary with binary segment maps for each agent\n",
    "        cell_size: 2D aggregation window size\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, np.ndarray]: dictionary with aggregated\n",
    "            binary segment maps for each agent\n",
    "    \"\"\"\n",
    "    grid_segment_map = {}\n",
    "    for k, v in segment_map.items():\n",
    "        grid_segment_map[k] = (get_region_grid(\n",
    "            region=v, cell_size=cell_size,\n",
    "            agg_fn=np.mean\n",
    "        ) > 0.5).astype(np.uint8)\n",
    "    return grid_segment_map\n",
    "\n",
    "\n",
    "def get_unreachable_regions(\n",
    "    segment_map: Dict[str, np.ndarray],\n",
    "    machines: Dict[str, Dict[str, np.ndarray]],\n",
    "    recyclers: Dict[str, Dict[str, np.ndarray]],\n",
    "    include_water: Optional[bool] = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Get unreachable regions\n",
    "\n",
    "    Get regions that cannot be accessed  by agents\n",
    "\n",
    "    Args:\n",
    "        segment_map: dictionary with binary segment maps for each agent\n",
    "        machines: placement information for fabricators\n",
    "        recyclers: placement information for recyclers\n",
    "        include_water: whether to include water segment\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: binary map with unreachable regions\n",
    "    \"\"\"\n",
    "    all_machines = np.zeros_like(segment_map['water'])\n",
    "    for a in machines.keys():\n",
    "        mm = machines[a]['map']\n",
    "        rm = recyclers[a]['map']\n",
    "        am = np.logical_or(mm, rm)\n",
    "        all_machines = np.logical_or(all_machines, am)\n",
    "    if include_water:\n",
    "        unreachable = np.logical_or(all_machines, segment_map['water'])\n",
    "    else:\n",
    "        unreachable = all_machines\n",
    "    return unreachable.astype(np.uint8)\n",
    "\n",
    "\n",
    "def spawn_resources_grid(\n",
    "    non_resource_grid: np.ndarray,\n",
    "    resource_prob: float,\n",
    "    rng: np.random.Generator\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Spawn initial resources\n",
    "\n",
    "    Spawn initial resources with uniform probability\n",
    "\n",
    "    Args:\n",
    "        non_resource_grid: binary map for regions where resources can\n",
    "            not be located\n",
    "        resource_prob: probability to spawn resource at a given point\n",
    "        rng: numpy random numbers generator\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: binary grid with spawned resources\n",
    "    \"\"\"\n",
    "    init_resource_prob_map = rng.uniform(size=non_resource_grid.shape)\n",
    "    init_resource_prob_map += non_resource_grid\n",
    "    resources_grid = (init_resource_prob_map < resource_prob)\n",
    "    return resources_grid.astype(np.uint8)\n",
    "\n",
    "\n",
    "def grid_to_center_mappings(\n",
    "    grid: np.ndarray,\n",
    "    cell_size: int\n",
    ") -> Tuple[Dict[tuple, np.ndarray], ...]:\n",
    "    \"\"\"Map grid to actual playing field\n",
    "\n",
    "    Map low dimensional grid to actual playing field\n",
    "\n",
    "    Args:\n",
    "        grid: low dimensional grid\n",
    "        cell_size: grid cell size on actual map\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[tuple, np.ndarray], ...]: mappings from\n",
    "            grid coordinates to actual map coordinates and\n",
    "            vise verse\n",
    "    \"\"\"\n",
    "    grid_loc = np.argwhere(grid > 0)\n",
    "    center_loc = grid_loc * cell_size + cell_size // 2\n",
    "    grid_to_center = {tuple(g): c for g, c in zip(grid_loc, center_loc)}\n",
    "    center_to_grid = {tuple(c): g for g, c in zip(grid_loc, center_loc)}\n",
    "    return grid_to_center, center_to_grid\n",
    "\n",
    "\n",
    "def get_template_texture(\n",
    "    cmap: np.ndarray,\n",
    "    template: np.ndarray,\n",
    "    color_eps: float,\n",
    "    rng: np.random.Generator\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Get template texture\n",
    "\n",
    "    Get template texture for map visualisation\n",
    "\n",
    "    Args:\n",
    "        cmap: background colormap (grass)\n",
    "        template: map template for dimensions\n",
    "        color_eps: color noise\n",
    "        rng: numpy random numbers generator\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: array with background texture\n",
    "    \"\"\"\n",
    "    color_noise = rng.uniform(\n",
    "        low=-color_eps, high=color_eps,\n",
    "        size=template.shape\n",
    "    ) * 255.\n",
    "    color_noise = np.round(color_noise, 0)[:, :, np.newaxis]\n",
    "    cmap = cmap[np.newaxis, np.newaxis, :]\n",
    "    texture = cmap + color_noise\n",
    "    texture = np.clip(texture, a_min=0, a_max=255)\n",
    "    texture *= template[:, :, np.newaxis]\n",
    "    return texture.astype(int)\n",
    "\n",
    "\n",
    "def get_thick_border(\n",
    "    borders: np.ndarray,\n",
    "    border_display_width: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Get thick segment borders\n",
    "\n",
    "    Get thick segment borders\n",
    "\n",
    "    Args:\n",
    "        borders: array with segments borders binary map\n",
    "        border_display_width: segments borders thickness\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: array with segment borders\n",
    "    \"\"\"\n",
    "    thick_borders = uniform_filter(\n",
    "        borders.astype(float),\n",
    "        size=border_display_width,\n",
    "        mode='constant'\n",
    "    )\n",
    "    return (thick_borders > 1e-5).astype(int)\n",
    "\n",
    "\n",
    "def get_machine_icon(\n",
    "    cmap: np.ndarray,\n",
    "    asset_size: int\n",
    ") -> ImageType:\n",
    "    \"\"\"Get fabricator icon\n",
    "\n",
    "    Get icon for fabricator\n",
    "\n",
    "    Args:\n",
    "        cmap: asset's color map\n",
    "        asset_size: asset's size in pixels\n",
    "\n",
    "    Returns:\n",
    "        ImageType: asset PIL image\n",
    "    \"\"\"\n",
    "    gen_size = 51\n",
    "    outline = 5\n",
    "    cmap = cmap[np.newaxis, np.newaxis, :]\n",
    "    machine = np.zeros(shape=(gen_size, gen_size, 3))\n",
    "    machine += cmap\n",
    "\n",
    "    vec_a = np.abs(np.arange(0, gen_size) - gen_size // 2)\n",
    "    vec_a -= vec_a.max()\n",
    "    vec_b = vec_a\n",
    "    dot = np.matmul(vec_a[:, np.newaxis], vec_b[np.newaxis, :])\n",
    "    dot = np.abs(dot / dot.max())\n",
    "    dot = (dot < 0.5).astype(int)\n",
    "    dot = dot[:, :, np.newaxis]\n",
    "\n",
    "    machine = machine * dot\n",
    "    machine[:outline, :, :] = 0\n",
    "    machine[-outline:, :, :] = 0\n",
    "    machine[:, :outline, :] = 0\n",
    "    machine[:, -outline:, :] = 0\n",
    "    machine = machine.astype(np.uint8)\n",
    "    image = Image.fromarray(machine)\n",
    "    machine_icon = image.resize(size=(asset_size, asset_size))\n",
    "    return machine_icon\n",
    "\n",
    "\n",
    "def create_circular_mask(\n",
    "    h: int, w: int,\n",
    "    center: Optional = None, radius: Optional = None\n",
    "):\n",
    "    \"\"\"Create circular mask\n",
    "    Args:\n",
    "        h: height\n",
    "        w: width\n",
    "        center: center\n",
    "        radius: radius\n",
    "    Returns:\n",
    "        np.ndarray: circular mask numpy array\n",
    "    \"\"\"\n",
    "    if center is None:\n",
    "        center = (int(w / 2), int(h / 2))\n",
    "    if radius is None:\n",
    "        radius = min(center[0], center[1], w - center[0], h - center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0]) ** 2 + (Y - center[1]) ** 2)\n",
    "\n",
    "    mask = dist_from_center <= radius\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_recycler_icon(\n",
    "    cmap: np.ndarray,\n",
    "    asset_size: int\n",
    ") -> ImageType:\n",
    "    \"\"\"Get recycler icon\n",
    "\n",
    "    Get icon for recycler\n",
    "\n",
    "    Args:\n",
    "        cmap: asset's color map\n",
    "        asset_size: asset's size in pixels\n",
    "\n",
    "    Returns:\n",
    "        ImageType: asset PIL image\n",
    "    \"\"\"\n",
    "    gen_size = 51\n",
    "    mask = create_circular_mask(\n",
    "        gen_size, gen_size)[:, :, np.newaxis]\n",
    "    cmap = cmap[np.newaxis, np.newaxis, :]\n",
    "    mask = cmap * mask\n",
    "    mask = mask.astype(np.uint8)\n",
    "    image = Image.fromarray(mask)\n",
    "    recycler_icon = image.resize(size=(asset_size, asset_size))\n",
    "    return recycler_icon\n",
    "\n",
    "\n",
    "def get_resource_icon(\n",
    "    asset_size: int,\n",
    "    cmap: np.ndarray = np.array([220, 255, 255]),\n",
    "    tolerance: int = 20\n",
    ") -> ImageType:\n",
    "    \"\"\"Get resource icon\n",
    "\n",
    "    Get icon for resource\n",
    "\n",
    "    Args:\n",
    "        cmap: asset's color map\n",
    "        asset_size: asset's size in pixels\n",
    "        tolerance: minimum pixel value to display\n",
    "\n",
    "    Returns:\n",
    "        ImageType: asset PIL image\n",
    "    \"\"\"\n",
    "    img = Image.new('RGB', (51, 51))\n",
    "    img = np.array(img)\n",
    "    row1, col1 = draw.polygon((1, 50, 35, 50), (25, 10, 25, 40))\n",
    "    row2, col2 = draw.polygon((20, 20, 34), (0, 50, 25))\n",
    "    img[row1, col1, :] = cmap\n",
    "    img[row2, col2, :] = cmap\n",
    "    img = Image.fromarray(img)\n",
    "    img = np.array(\n",
    "        img.resize(size=(asset_size, asset_size))\n",
    "    )\n",
    "    filter = np.argwhere(img.mean(axis=2) < tolerance)\n",
    "    img[filter[:, 0], filter[:, 1], :] = 0\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def get_trash_icon(\n",
    "    asset_size: int,\n",
    "    cmap: np.ndarray = np.array([94, 45, 1])\n",
    ") -> ImageType:\n",
    "    \"\"\"Get trash icon\n",
    "\n",
    "    Get icon for trash\n",
    "\n",
    "    Args:\n",
    "        cmap: asset's color map\n",
    "        asset_size: asset's size in pixels\n",
    "\n",
    "    Returns:\n",
    "        ImageType: asset PIL image\n",
    "    \"\"\"\n",
    "    img = np.ones(shape=(asset_size, asset_size, 3))\n",
    "    img = img * cmap[np.newaxis, np.newaxis, :]\n",
    "    return Image.fromarray(img.astype(np.uint8))\n",
    "\n",
    "\n",
    "def get_agent_icon(\n",
    "    asset_size: int,\n",
    "    cmap: np.ndarray,\n",
    "    tolerance: int = 20\n",
    ") -> ImageType:\n",
    "    \"\"\"Get agent icon\n",
    "\n",
    "    Get icon for agent\n",
    "\n",
    "    Args:\n",
    "        cmap: asset's color map\n",
    "        asset_size: asset's size in pixels\n",
    "        tolerance: minimum pixel value to display\n",
    "\n",
    "    Returns:\n",
    "        ImageType: asset PIL image\n",
    "    \"\"\"\n",
    "    img = Image.new('RGB', (51, 51))\n",
    "    img = np.array(img)\n",
    "    row1, col1 = draw.polygon((1, 50, 50, 40, 40, 50, 50),\n",
    "                              (25, 1, 18, 18, 32, 32, 50))\n",
    "    img[row1, col1, :] = cmap\n",
    "    img = Image.fromarray(img)\n",
    "    img = np.array(\n",
    "        img.resize(size=(asset_size, asset_size))\n",
    "    )\n",
    "    filter = np.argwhere(img.mean(axis=2) < tolerance)\n",
    "    img[filter[:, 0], filter[:, 1], :] = 0\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def get_arrow_icon(\n",
    "    asset_size: int,\n",
    "    cmap: np.ndarray,\n",
    "    tolerance: int = 10\n",
    ") -> ImageType:\n",
    "    \"\"\"Get arrow icon\n",
    "\n",
    "    Get icon for arrow\n",
    "\n",
    "    Args:\n",
    "        cmap: asset's color map\n",
    "        asset_size: asset's size in pixels\n",
    "        tolerance: minimum pixel value to display\n",
    "\n",
    "    Returns:\n",
    "        ImageType: asset PIL image\n",
    "    \"\"\"\n",
    "    img = Image.new('RGB', (51, 51))\n",
    "    img = np.array(img)\n",
    "    row1, col1 = draw.polygon((0, 50, 35, 50), (25, 14, 25, 36))\n",
    "    img[row1, col1, :] = cmap\n",
    "    img = Image.fromarray(img)\n",
    "    img = np.array(\n",
    "        img.resize(size=(asset_size, asset_size))\n",
    "    )\n",
    "    filter = np.argwhere(img.mean(axis=2) < tolerance)\n",
    "    img[filter[:, 0], filter[:, 1], :] = 0\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def create_icons(\n",
    "    segment_map: Dict[str, np.ndarray],\n",
    "    machine_icon_size: int,\n",
    "    agent_icon_size: int,\n",
    "    arrow_icon_size: int,\n",
    "    resource_icon_size: int,\n",
    "    trash_icon_size: int,\n",
    "    rng: np.random.Generator,\n",
    "    color_maps: Dict[str, np.ndarray] = COLOR_MAPS,\n",
    "    resource_cmap: np.ndarray = np.array([220, 255, 255]),\n",
    "    trash_cmap: np.ndarray = np.array([94, 45, 1]),\n",
    "    north_arrow_cmap: np.ndarray = np.array([255, 1, 1]),\n",
    "    home_arrow_cmap: np.ndarray = np.array([1, 255, 1]),\n",
    "    non_home_arrow_cmap: np.ndarray = np.array([1, 1, 255]),\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Get icons\n",
    "\n",
    "    Get all icons required for visualisation\n",
    "\n",
    "    Args:\n",
    "        segment_map: dictionary with binary segment maps for each agent\n",
    "        machine_icon_size: machine icon size in pixels\n",
    "        agent_icon_size: agent icon size in pixels\n",
    "        arrow_icon_size: arrow icon size in pixels\n",
    "        resource_icon_size: resource icon size in pixels\n",
    "        trash_icon_size: trash icon size in pixels\n",
    "        rng: numpy random number generator\n",
    "        color_maps: color maps for each agent\n",
    "        resource_cmap: resource color map\n",
    "        trash_cmap: trash color map\n",
    "        north_arrow_cmap: north arrow color map\n",
    "        home_arrow_cmap: home arrow color map\n",
    "        non_home_arrow_cmap: non home arrow color map\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: dictionary with icons for each asset\n",
    "    \"\"\"\n",
    "    icons = {}\n",
    "    agents = [a for a in segment_map.keys() if a != 'water']\n",
    "    # rng.shuffle(agents)\n",
    "    color_reference = {a: c for a, c in zip(agents, color_maps)}\n",
    "    color_reference['domestic'] = 'domestic'\n",
    "    color_reference['foreign'] = 'foreign'\n",
    "    for a, c in color_reference.items():\n",
    "        icons[a] = {}\n",
    "        icons[a]['machine'] = get_machine_icon(\n",
    "            cmap=color_maps[c],\n",
    "            asset_size=machine_icon_size\n",
    "        )\n",
    "        icons[a]['recycler'] = get_recycler_icon(\n",
    "            cmap=color_maps[c],\n",
    "            asset_size=machine_icon_size\n",
    "        )\n",
    "        icons[a]['agent'] = get_agent_icon(\n",
    "            cmap=color_maps[c],\n",
    "            asset_size=agent_icon_size\n",
    "        )\n",
    "        icons[a]['cmap'] = color_maps[c]\n",
    "    icons['resource'] = get_resource_icon(\n",
    "        cmap=resource_cmap,\n",
    "        asset_size=resource_icon_size\n",
    "    )\n",
    "    icons['trash'] = get_trash_icon(\n",
    "        cmap=trash_cmap,\n",
    "        asset_size=trash_icon_size\n",
    "    )\n",
    "    icons['north_arrow'] = get_arrow_icon(\n",
    "        cmap=north_arrow_cmap,\n",
    "        asset_size=arrow_icon_size\n",
    "    )\n",
    "    icons['home_arrow'] = get_arrow_icon(\n",
    "        cmap=home_arrow_cmap,\n",
    "        asset_size=arrow_icon_size\n",
    "    )\n",
    "    icons['non_home_arrow'] = get_arrow_icon(\n",
    "        cmap=non_home_arrow_cmap,\n",
    "        asset_size=arrow_icon_size\n",
    "    )\n",
    "    return icons\n",
    "\n",
    "\n",
    "def create_erasers(\n",
    "    agent_size: int,\n",
    "    arrow_size: int,\n",
    "    resource_size: int,\n",
    "    trash_size: int\n",
    ") -> Dict[str, ImageType]:\n",
    "    \"\"\"Create erasers\n",
    "\n",
    "    Create zero-valued templates for removing icons\n",
    "    from respective image layer\n",
    "\n",
    "    Args:\n",
    "        agent_size: agent icon size in pixels\n",
    "        arrow_size: arrow icon size in pixels\n",
    "        resource_size: resource icon size in pixels\n",
    "        trash_size: trash icon size in pixels\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, ImageType]: dictionary with\n",
    "            zero-valued templates for each asset\n",
    "    \"\"\"\n",
    "    erasers = {}\n",
    "    erasers['agent'] = Image.fromarray(\n",
    "        np.zeros(shape=(agent_size, agent_size))\n",
    "    )\n",
    "    erasers['arrow'] = Image.fromarray(\n",
    "        np.zeros(shape=(arrow_size, arrow_size))\n",
    "    )\n",
    "    erasers['resource'] = Image.fromarray(\n",
    "        np.zeros(shape=(resource_size, resource_size))\n",
    "    )\n",
    "    erasers['trash'] = Image.fromarray(\n",
    "        np.zeros(shape=(trash_size, trash_size))\n",
    "    )\n",
    "    return erasers\n",
    "\n",
    "\n",
    "def create_substrate_texture(\n",
    "    segment_map: Dict[str, np.ndarray],\n",
    "    borders: np.ndarray,\n",
    "    border_display_width: int,\n",
    "    machines: Dict[str, np.ndarray],\n",
    "    recyclers: Dict[str, np.ndarray],\n",
    "    icons: Dict[str, Any],\n",
    "    rng: np.random.Generator\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Create substrate texture\n",
    "\n",
    "    Create substrate texture for state visualisation\n",
    "\n",
    "    Args:\n",
    "        segment_map: dictionary with binary segment maps for each agent\n",
    "        borders: array with segments borders binary map\n",
    "        border_display_width: segments borders thickness\n",
    "        machines: placement information for fabricators\n",
    "        recyclers: placement information for recyclers\n",
    "        icons: dictionary with icons for each asset\n",
    "        rng: numpy random number generator\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: substrate texture image array\n",
    "    \"\"\"\n",
    "    grass_texture = get_template_texture(\n",
    "        cmap=np.array([1, 128, 1]),\n",
    "        template=np.logical_not(segment_map['water']),\n",
    "        color_eps=0.075,\n",
    "        rng=rng\n",
    "    )\n",
    "    water_texture = get_template_texture(\n",
    "        cmap=np.array([120, 140, 250]),\n",
    "        template=segment_map['water'],\n",
    "        color_eps=0.04,\n",
    "        rng=rng\n",
    "    )\n",
    "    thick_borders = get_thick_border(\n",
    "        borders=borders,\n",
    "        border_display_width=border_display_width\n",
    "    )\n",
    "    borders_texture = get_template_texture(\n",
    "        cmap=np.array([150, 150, 150]),\n",
    "        template=thick_borders,\n",
    "        color_eps=0.075,\n",
    "        rng=rng\n",
    "    )\n",
    "\n",
    "    final_texture = grass_texture + water_texture\n",
    "    final_texture *= np.logical_not(thick_borders)[:, :, np.newaxis]\n",
    "    final_texture += borders_texture\n",
    "    final_texture = Image.fromarray(final_texture.astype(np.uint8))\n",
    "    # Add machines and recycler assets\n",
    "    for a in segment_map.keys():\n",
    "        if a != 'water':\n",
    "            m_icon = icons[a]['machine']\n",
    "            m_offset = machines[a]['center'] - (m_icon.size[0] // 2)\n",
    "            r_icon = icons[a]['recycler']\n",
    "            r_offset = recyclers[a]['center'] - (r_icon.size[0] // 2)\n",
    "            final_texture.paste(m_icon, tuple(m_offset)[::-1])\n",
    "            final_texture.paste(r_icon, tuple(r_offset)[::-1])\n",
    "    return np.array(final_texture).astype(np.uint8)\n",
    "\n",
    "\n",
    "def get_agents_perspective(\n",
    "    grid_size: int,\n",
    "    segment_map: Dict[str, np.ndarray],\n",
    "    icons: Dict[str, Any],\n",
    "    machines: Dict[str, Any],\n",
    "    recyclers: Dict[str, Any],\n",
    "    obs_dim: int\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Get agents perspective\n",
    "\n",
    "    Get machines attributions for the local view of\n",
    "    each agent\n",
    "\n",
    "    Args:\n",
    "        grid_size: 2D square game field size\n",
    "        segment_map: dictionary with binary segment maps for each agent\n",
    "        machines: placement information for fabricators\n",
    "        recyclers: placement information for recyclers\n",
    "        icons: dictionary with icons for each asset\n",
    "        obs_dim: local visual observation size\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, np.ndarray]: binary maps for foreign and\n",
    "            domestic machines for each agent\n",
    "    \"\"\"\n",
    "    p = obs_dim\n",
    "    machines_by_agent = {}\n",
    "    valid_agents = [a for a in segment_map.keys() if a != 'water']\n",
    "    for a1 in valid_agents:\n",
    "        mba = Image.new('RGB', (grid_size, grid_size))\n",
    "        for a2 in valid_agents:\n",
    "            if a1 != a2:\n",
    "                m_icon = icons['foreign']['machine']\n",
    "                r_icon = icons['foreign']['recycler']\n",
    "            else:\n",
    "                m_icon = icons['domestic']['machine']\n",
    "                r_icon = icons['domestic']['recycler']\n",
    "            m_offset = machines[a2]['center'] - (m_icon.size[0] // 2)\n",
    "            r_offset = recyclers[a2]['center'] - (r_icon.size[0] // 2)\n",
    "            mba.paste(m_icon, tuple(m_offset)[::-1])\n",
    "            mba.paste(r_icon, tuple(r_offset)[::-1])\n",
    "        machines_by_agent[a1] = np.pad(\n",
    "            np.array(mba), pad_width=((p, p), (p, p), (0, 0)),\n",
    "            constant_values=0)\n",
    "    return machines_by_agent\n",
    "\n",
    "\n",
    "def render_init_resources(\n",
    "    resource_grid_to_center: Dict[tuple, np.ndarray],\n",
    "    icons: Dict[str, Any],\n",
    "    grid_size: int\n",
    ") -> ImageType:\n",
    "    \"\"\"Render initial resources\n",
    "\n",
    "    Render initial resources distribution\n",
    "\n",
    "    Args:\n",
    "        grid_size: 2D square game field size\n",
    "        icons: dictionary with icons for each asset\n",
    "        resource_grid_to_center: mapping from low dimension\n",
    "            resource grid to full size game field\n",
    "\n",
    "    Returns:\n",
    "        ImageType: PIL image with initial resources\n",
    "    \"\"\"\n",
    "    resource_map = np.zeros(shape=(grid_size, grid_size, 3))\n",
    "    resource_map = Image.fromarray(resource_map.astype(np.uint8))\n",
    "    resources_loc = np.array(list(resource_grid_to_center.values()))\n",
    "    r_icon = np.array(icons['resource'])\n",
    "    mask = (r_icon.sum(axis=-1) > 0) * 255.\n",
    "    mask = (mask).astype(np.uint8)\n",
    "    mask = Image.fromarray(mask)\n",
    "    r_icon = Image.fromarray(r_icon.astype(np.uint8))\n",
    "    for rl in resources_loc:\n",
    "        offset = rl - (r_icon.size[0] // 2)\n",
    "        offset = tuple(offset)[::-1]\n",
    "        resource_map.paste(r_icon, offset, mask=mask)\n",
    "    return resource_map\n",
    "\n",
    "\n",
    "def render_wealth(\n",
    "    money: int,\n",
    "    block_size: int\n",
    ") -> ImageType:\n",
    "    \"\"\"Render wealth for local menu\n",
    "\n",
    "    Render wealth for local menu (deprecated)\n",
    "\n",
    "    Args:\n",
    "        money: current agent's wealth\n",
    "        block_size: menu block size\n",
    "\n",
    "    Returns:\n",
    "        ImageType: PIL image wealth size\n",
    "    \"\"\"\n",
    "    wealth = Image.new('RGB',\n",
    "                       (block_size * 2, block_size))\n",
    "    dw = ImageDraw.Draw(wealth)\n",
    "    dw.text((1, 1), str(money), fill=(255, 255, 255))\n",
    "    return wealth\n",
    "\n",
    "\n",
    "def init_agents(\n",
    "    centers: Dict[str, np.ndarray],\n",
    "    block_size: int,\n",
    "    rng: np.random.Generator\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Init agents state\n",
    "\n",
    "    Create dictionary with agents initial state\n",
    "\n",
    "    Args:\n",
    "        block_size: menu block size\n",
    "        centers: segment centers locations\n",
    "        rng: numpy random number generator\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: agents initial state\n",
    "    \"\"\"\n",
    "    agents_state = {}\n",
    "    dirs = [0, 0.5, 1, 1.5]\n",
    "    for a, c in centers.items():\n",
    "        agents_state[a] = {}\n",
    "        agents_state[a]['loc'] = c\n",
    "        agents_state[a]['dir_pi'] = rng.choice(dirs, 1).item()\n",
    "        agents_state[a]['inventory'] = {}\n",
    "        agents_state[a]['inventory']['resource'] = False\n",
    "        agents_state[a]['inventory']['trash'] = False\n",
    "        agents_state[a]['inventory']['trash_source'] = None\n",
    "        agents_state[a]['wealth'] = 0\n",
    "        agents_state[a]['last_render_wealth'] = 0\n",
    "        agents_state[a]['last_wealth_image'] = render_wealth(\n",
    "            money=0, block_size=block_size\n",
    "        )\n",
    "    return agents_state\n",
    "\n",
    "\n",
    "def pi_to_rad(n_pi: float) -> int:\n",
    "    \"\"\"Convert pi to radians\n",
    "\n",
    "    Convert pi to radian angle for rotations\n",
    "\n",
    "    Args:\n",
    "        n_pi: angle in arc notation\n",
    "\n",
    "    Returns:\n",
    "        int: angle in radians\n",
    "    \"\"\"\n",
    "    return int((n_pi - 0.5) * 180)\n",
    "\n",
    "\n",
    "def render_agents(\n",
    "    agents_state: Dict[str, Any],\n",
    "    icons: Dict[str, Any],\n",
    "    grid_size: int,\n",
    "    local_mode: Optional[bool] = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Render agents\n",
    "\n",
    "    Create image layer with agents\n",
    "\n",
    "    Args:\n",
    "        agents_state: agents state dictionary\n",
    "        grid_size: 2D square game field size\n",
    "        icons: dictionary with icons for each asset\n",
    "        local_mode: whether to generate subjective view\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: array with agents image layer\n",
    "    \"\"\"\n",
    "    agents_map = Image.new('RGB', (grid_size, grid_size))\n",
    "    for a, s in agents_state.items():\n",
    "        if not local_mode:\n",
    "            icon = icons[a]['agent']\n",
    "        else:\n",
    "            icon = icons['foreign']['agent']\n",
    "        icon = icon.rotate(pi_to_rad(s['dir_pi']))\n",
    "        y, x = tuple(s['loc'])\n",
    "        x, y = x - icon.size[0] // 2, y - icon.size[1] // 2\n",
    "        agents_map.paste(icon, (x, y))\n",
    "    return np.array(agents_map)\n",
    "\n",
    "\n",
    "def loc_to_coord(\n",
    "    loc: np.ndarray,\n",
    "    grid_size: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Numpy loc to coordinates\n",
    "\n",
    "    Numpy loc to Cartesian coordinates\n",
    "\n",
    "    Args:\n",
    "        loc: array with numpy loc\n",
    "        grid_size: game field grid size\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: array with Cartesian coordinates\n",
    "    \"\"\"\n",
    "    y, x = tuple(loc)\n",
    "    y = grid_size - y\n",
    "    return np.array([x, y])\n",
    "\n",
    "\n",
    "def cart2pol(x: int, y: int) -> Tuple[float, float]:\n",
    "    \"\"\"Cartesian to polar\n",
    "\n",
    "    Convert Cartesian coordinates to polar coordinates\n",
    "\n",
    "    Args:\n",
    "        x: x-coordinate\n",
    "        y: y-coordinate\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: (rho, phi) - polar coordinates\n",
    "    \"\"\"\n",
    "    rho = np.sqrt(x ** 2 + y ** 2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return rho, phi\n",
    "\n",
    "\n",
    "def dir_to_shift(dir: float) -> np.ndarray:\n",
    "    \"\"\"Direction to shift\n",
    "\n",
    "    Utility function for building local observation\n",
    "\n",
    "    Args:\n",
    "        dir: direction in arc notation\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: array with coordinates shift\n",
    "    \"\"\"\n",
    "    if dir == 0.0:\n",
    "        shift = np.array([0, 1])\n",
    "    elif dir == 0.5:\n",
    "        shift = np.array([-1, 0])\n",
    "    elif dir == 1.0:\n",
    "        shift = np.array([0, -1])\n",
    "    elif dir == 1.5:\n",
    "        shift = np.array([1, 0])\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f'Invalid direction for movement: {dir} * pi'\n",
    "        )\n",
    "    return shift\n",
    "\n",
    "\n",
    "class GameEngine:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seed: int,\n",
    "        grid_size: int = 210,  # should be div by resource_size and trash_size\n",
    "        obs_dim: int = 60,  # should be divisible by 5\n",
    "        move_step: int = 7,\n",
    "        resource_price: int = 10,\n",
    "        recycle_cost: int = 4,\n",
    "        border_distort_range: Tuple[int, int] = (-1, 2),\n",
    "        max_edge_dev: float = 0.1,\n",
    "        max_tries: int = 25,\n",
    "        machine_size: int = 9,\n",
    "        machine_reach: int = 9,\n",
    "        agent_size: int = 9,\n",
    "        agent_reach: int = 9,\n",
    "        resource_size: int = 7,\n",
    "        trash_size: int = 5,\n",
    "        resource_prob: float = 0.1,\n",
    "        border_display_width: int = 2,\n",
    "        blocked_vanish_alpha: float = 0.25\n",
    "    ):\n",
    "        \"\"\"Game Engine\n",
    "\n",
    "        Game engine for AIJ Multi-Agent AI competition\n",
    "\n",
    "        Valid action space (discrete):\n",
    "            # 0: move forward by `move_step` pixels if possible\n",
    "            # 1: move left by `move_step` pixels if possible\n",
    "            # 2: move right by `move_step` pixels if possible\n",
    "            # 3: move backward by `move_step` pixels if possible\n",
    "            # 4: pickup resource (if closer than `agent_reach` pixels)\n",
    "            # 5: pickup trash (if closer than `agent_reach` pixels)\n",
    "            # 6: throw resource (put into machine if closer than `machine_reach`)\n",
    "            # 7: throw trash (put into recycler if closer than `machine_reach`)\n",
    "            # 8: noop\n",
    "\n",
    "        Parameters:\n",
    "            seed: random seed for engine\n",
    "            grid_size: 2D square game field size\n",
    "            obs_dim: local visual observation size\n",
    "            move_step: movement step size in pixels\n",
    "            resource_price: reward given for resource processing\n",
    "            recycle_cost: cost of recycling trash\n",
    "            border_distort_range: noise range for borders distortion\n",
    "            max_edge_dev: maximum segment border shift\n",
    "            max_tries: maximum attempts for border generation\n",
    "            machine_size: machine icon size in pixels\n",
    "            machine_reach: size of machine interaction region\n",
    "            agent_size: agent icon size in pixels\n",
    "            agent_reach: agent reach when picking up items\n",
    "            resource_size: resource icon size in pixels\n",
    "            trash_size: trash icon size in pixels\n",
    "            resource_prob: probability to spawn resource at a given point\n",
    "            border_display_width: segments borders thickness\n",
    "            blocked_vanish_alpha: blocked segment fogging degree\n",
    "        \"\"\"\n",
    "        assert grid_size % resource_size == 0 and grid_size % trash_size == 0\n",
    "        assert obs_dim % 5 == 0\n",
    "        assert obs_dim // 5 >= resource_size + trash_size\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.grid_size = grid_size\n",
    "        self.diag = np.sqrt(2 * grid_size ** 2).item()\n",
    "        self.move_step = move_step\n",
    "        self.obs_dim = obs_dim\n",
    "        self.block_size = obs_dim // 5\n",
    "        self.resource_price = resource_price\n",
    "        self.recycle_cost = recycle_cost\n",
    "        self.border_distort_range = border_distort_range\n",
    "        self.max_edge_dev = max_edge_dev\n",
    "        self.max_tries = max_tries\n",
    "        self.machine_size = machine_size\n",
    "        self.machine_reach = machine_reach\n",
    "        self.agent_size = agent_size\n",
    "        self.agent_reach = agent_reach\n",
    "        self.arrow_size = obs_dim // 5\n",
    "        self.resource_size = resource_size\n",
    "        self.trash_size = trash_size\n",
    "        self.resource_prob = resource_prob\n",
    "        self.border_display_width = border_display_width\n",
    "        self.blocked_vanish_alpha = blocked_vanish_alpha\n",
    "\n",
    "        # Construct main layout\n",
    "        self.h_borders, self.v_borders = create_borders(\n",
    "            grid_size=grid_size,\n",
    "            border_distort_range=border_distort_range,\n",
    "            max_edge_dev=max_edge_dev,\n",
    "            max_tries=max_tries,\n",
    "            rng=self.rng\n",
    "        )\n",
    "        self.borders = np.logical_or(self.h_borders, self.v_borders)\n",
    "        self.segment_map = get_segment_map(\n",
    "            h_borders=self.h_borders, v_borders=self.v_borders\n",
    "        )\n",
    "        self.machines, self.recyclers, self.centers = get_all_machines_loc(\n",
    "            segment_map=self.segment_map,\n",
    "            machine_size=machine_size,\n",
    "            rng=self.rng\n",
    "        )\n",
    "        # Segment maps for resources and trash\n",
    "        self.resource_segment_map = get_segment_map_grid(\n",
    "            segment_map=self.segment_map,\n",
    "            cell_size=resource_size\n",
    "        )\n",
    "        self.trash_segment_map = get_segment_map_grid(\n",
    "            segment_map=self.segment_map,\n",
    "            cell_size=trash_size\n",
    "        )\n",
    "\n",
    "        # Distribute initial resources\n",
    "        non_resource = get_non_resource_regions(\n",
    "            segment_map=self.segment_map,\n",
    "            machine_size=machine_size,\n",
    "            machines=self.machines,\n",
    "            recyclers=self.recyclers,\n",
    "        )\n",
    "        self.non_resource_grid = get_region_grid(\n",
    "            region=non_resource,\n",
    "            cell_size=resource_size\n",
    "        )\n",
    "        self.resource_grid = spawn_resources_grid(\n",
    "            non_resource_grid=self.non_resource_grid,\n",
    "            resource_prob=resource_prob,\n",
    "            rng=self.rng\n",
    "        )\n",
    "        mappings = grid_to_center_mappings(\n",
    "            grid=self.resource_grid,\n",
    "            cell_size=resource_size\n",
    "        )\n",
    "        self.resource_grid_to_center, self.resource_center_to_grid = mappings\n",
    "\n",
    "        # Create grid for trash\n",
    "        self.non_trash_grid = get_region_grid(\n",
    "            region=non_resource,\n",
    "            cell_size=trash_size\n",
    "        )\n",
    "        self.trash_grid = np.zeros_like(\n",
    "            self.non_trash_grid\n",
    "        ).astype(np.uint8)\n",
    "        self.trash_grid_to_center = {}\n",
    "        self.trash_center_to_grid = {}\n",
    "        self.trash_center_to_source = {}\n",
    "\n",
    "        # Get unreachable regions\n",
    "        self.non_reachable = get_unreachable_regions(\n",
    "            segment_map=self.segment_map,\n",
    "            machines=self.machines,\n",
    "            recyclers=self.recyclers,\n",
    "        )\n",
    "        self.all_machines = get_unreachable_regions(\n",
    "            segment_map=self.segment_map,\n",
    "            machines=self.machines,\n",
    "            recyclers=self.recyclers,\n",
    "            include_water=False\n",
    "        )[:, :, np.newaxis]\n",
    "        self.all_machines = self.pad_state(self.all_machines)\n",
    "\n",
    "        # Generate assets for rendering\n",
    "        self.icons = create_icons(\n",
    "            segment_map=self.segment_map,\n",
    "            machine_icon_size=self.machine_size,\n",
    "            agent_icon_size=self.agent_size,\n",
    "            arrow_icon_size=self.arrow_size,\n",
    "            resource_icon_size=self.resource_size,\n",
    "            trash_icon_size=self.trash_size,\n",
    "            rng=self.rng\n",
    "        )\n",
    "\n",
    "        # Generate erasers for assets\n",
    "        self.erasers = create_erasers(\n",
    "            agent_size=self.agent_size,\n",
    "            arrow_size=self.arrow_size,\n",
    "            resource_size=self.resource_size,\n",
    "            trash_size=self.trash_size\n",
    "        )\n",
    "\n",
    "        # Generate main substrate texture\n",
    "        self.substrate_texture = create_substrate_texture(\n",
    "            segment_map=self.segment_map,\n",
    "            borders=self.borders,\n",
    "            border_display_width=border_display_width,\n",
    "            machines=self.machines,\n",
    "            recyclers=self.recyclers,\n",
    "            icons=self.icons,\n",
    "            rng=self.rng\n",
    "        )\n",
    "        self.agents_perspectives = get_agents_perspective(\n",
    "            grid_size=self.grid_size,\n",
    "            segment_map=self.segment_map,\n",
    "            icons=self.icons,\n",
    "            machines=self.machines,\n",
    "            recyclers=self.recyclers,\n",
    "            obs_dim=self.obs_dim\n",
    "        )\n",
    "\n",
    "        # Generate resources layer\n",
    "        self.resource_map = render_init_resources(\n",
    "            resource_grid_to_center=self.resource_grid_to_center,\n",
    "            icons=self.icons,\n",
    "            grid_size=grid_size\n",
    "        )\n",
    "\n",
    "        # Generate trash layer\n",
    "        self.trash_map = Image.new('RGB', (grid_size, grid_size))\n",
    "\n",
    "        # Create agents state\n",
    "        self.agents_state = init_agents(\n",
    "            centers=self.centers,\n",
    "            block_size=self.block_size,\n",
    "            rng=self.rng\n",
    "        )\n",
    "\n",
    "        # Create blocked dict\n",
    "        self.blocked = {a: False for a in self.agents_state.keys()}\n",
    "        self.blocked_map = np.zeros(\n",
    "            shape=(grid_size, grid_size)).astype(np.uint8)\n",
    "\n",
    "    def agents_map(self, local_mode: Optional[bool] = False) -> np.ndarray:\n",
    "        \"\"\"Render agents\n",
    "\n",
    "        Create image layer with agents\n",
    "\n",
    "        Args:\n",
    "            local_mode: whether to generate subjective view\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: array with agents image layer\n",
    "        \"\"\"\n",
    "        return render_agents(\n",
    "            agents_state=self.agents_state,\n",
    "            icons=self.icons,\n",
    "            grid_size=self.grid_size,\n",
    "            local_mode=local_mode\n",
    "        )\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        \"\"\"Get state\n",
    "\n",
    "        Get current game engine state\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: image array with current visual state\n",
    "        \"\"\"\n",
    "        resource_map = np.array(self.resource_map)\n",
    "        trash_map = np.array(self.trash_map)\n",
    "        # add resources layer\n",
    "        state = self.substrate_texture * np.logical_not(\n",
    "            resource_map > 0)\n",
    "        state = state + resource_map\n",
    "        # add trash layer\n",
    "        state = state * np.logical_not(\n",
    "            trash_map > 0)\n",
    "        state = state + trash_map\n",
    "        # add agents layer\n",
    "        agents_map = self.agents_map()\n",
    "        state = state * np.logical_not(\n",
    "            agents_map > 0)\n",
    "        state = state + agents_map\n",
    "        return state\n",
    "\n",
    "    def trash_by_segment(self, agent_id: str) -> int:\n",
    "        \"\"\"Calculate trash by segment\n",
    "\n",
    "        Calculate number of trash items on a given\n",
    "        segment\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            int: number of trash items on a given segment\n",
    "        \"\"\"\n",
    "        segment = self.trash_segment_map[agent_id]\n",
    "        trash_by_segment = np.logical_and(\n",
    "            segment, self.trash_grid\n",
    "        )\n",
    "        return trash_by_segment.sum()\n",
    "\n",
    "    def resource_by_segment(self, agent_id: str) -> int:\n",
    "        \"\"\"Calculate resource by segment\n",
    "\n",
    "        Calculate number of resource items on a given\n",
    "        segment\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            int: number of resource items on a given segment\n",
    "        \"\"\"\n",
    "        segment = self.resource_segment_map[agent_id]\n",
    "        resource_by_segment = np.logical_and(\n",
    "            segment, self.resource_grid\n",
    "        )\n",
    "        return resource_by_segment.sum()\n",
    "\n",
    "    def is_home_segment(self, agent_id) -> bool:\n",
    "        \"\"\"Check if home segment\n",
    "\n",
    "        Check if given agent is on home segment\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            bool: True if currently on home segment\n",
    "        \"\"\"\n",
    "        loc = self.agents_state[agent_id]['loc']\n",
    "        home = self.segment_map[agent_id][loc[0], loc[1]]\n",
    "        return bool(home.item())\n",
    "\n",
    "    def add_block(self, agent_id: str) -> None:\n",
    "        \"\"\"Add block\n",
    "\n",
    "        Impose block on agent actions (resource and trash\n",
    "        recycling)\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if not self.blocked[agent_id]:\n",
    "            self.blocked[agent_id] = True\n",
    "            self.blocked_map = np.logical_or(\n",
    "                self.blocked_map, self.segment_map[agent_id])\n",
    "            blocked_segment = np.logical_not(self.segment_map[agent_id])\n",
    "            blocked_segment = blocked_segment + self.blocked_vanish_alpha\n",
    "            blocked_segment = np.clip(blocked_segment, a_min=0, a_max=1)\n",
    "            blocked_segment = np.expand_dims(blocked_segment, axis=-1)\n",
    "            new_st = self.substrate_texture * blocked_segment\n",
    "            new_st = np.round(new_st, 0).astype(np.uint8)\n",
    "            self.substrate_texture = new_st\n",
    "\n",
    "    def pad_state(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Pad state\n",
    "\n",
    "        Pad image state for building local observations\n",
    "\n",
    "        Args:\n",
    "            state: raw engine visual state\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: padded engine visual state\n",
    "        \"\"\"\n",
    "        p = self.obs_dim\n",
    "        state_pad = np.pad(\n",
    "            state, pad_width=((p, p), (p, p), (0, 0)),\n",
    "            constant_values=0)\n",
    "        return state_pad\n",
    "\n",
    "    def delete_resource(self, center_loc: tuple) -> None:\n",
    "        \"\"\"Delete resource\n",
    "\n",
    "        Delete resource from game grid given its raw\n",
    "        numpy loc\n",
    "\n",
    "        Args:\n",
    "            center_loc: numpy loc on the 2D game field\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # delete from the references\n",
    "        grid_loc = tuple(self.resource_center_to_grid[center_loc])\n",
    "        del self.resource_center_to_grid[center_loc]\n",
    "        del self.resource_grid_to_center[grid_loc]\n",
    "        # delete from the grid\n",
    "        self.resource_grid[grid_loc[0], grid_loc[1]] = 0\n",
    "        # delete from the resource map\n",
    "        y, x = center_loc\n",
    "        c = (self.resource_size // 2)\n",
    "        self.resource_map.paste(\n",
    "            self.erasers['resource'], (x - c, y - c)\n",
    "        )\n",
    "\n",
    "    def delete_trash(self, center_loc: tuple) -> str:\n",
    "        \"\"\"Delete trash\n",
    "\n",
    "        Delete trash from game grid given its raw\n",
    "        numpy loc\n",
    "\n",
    "        Args:\n",
    "            center_loc: numpy loc on the 2D game field\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # delete from the references\n",
    "        grid_loc = tuple(self.trash_center_to_grid[center_loc])\n",
    "        del self.trash_center_to_grid[center_loc]\n",
    "        del self.trash_grid_to_center[grid_loc]\n",
    "        trash_source = self.trash_center_to_source[center_loc]\n",
    "        del self.trash_center_to_source[center_loc]\n",
    "        # delete from the grid\n",
    "        self.trash_grid[grid_loc[0], grid_loc[1]] = 0\n",
    "        # delete from the trash map\n",
    "        y, x = center_loc\n",
    "        c = (self.trash_size // 2)\n",
    "        self.trash_map.paste(\n",
    "            self.erasers['trash'], (x - c, y - c)\n",
    "        )\n",
    "        return trash_source\n",
    "\n",
    "    def add_resource(self, grid_loc: np.ndarray) -> None:\n",
    "        \"\"\"Add resource\n",
    "\n",
    "        Add resource to the game field and resource grid\n",
    "        given its resource grid location\n",
    "\n",
    "        Args:\n",
    "            grid_loc: numpy loc resource grid\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        center_loc = grid_loc * self.resource_size + self.resource_size // 2\n",
    "        # add to the references\n",
    "        self.resource_grid_to_center[tuple(grid_loc)] = center_loc\n",
    "        self.resource_center_to_grid[tuple(center_loc)] = grid_loc\n",
    "        # add to the resource grid\n",
    "        self.resource_grid[grid_loc[0], grid_loc[1]] = 1\n",
    "        # add to the resource map\n",
    "        y, x = tuple(center_loc)\n",
    "        c = (self.resource_size // 2)\n",
    "        self.resource_map.paste(\n",
    "            self.icons['resource'], (x - c, y - c)\n",
    "        )\n",
    "\n",
    "    def add_trash(self, grid_loc: np.ndarray, agent_id: str) -> None:\n",
    "        \"\"\"Add trash\n",
    "\n",
    "        Add trash to the game field and trash grid\n",
    "        given its trash grid location\n",
    "\n",
    "        Args:\n",
    "            grid_loc: numpy loc trash grid\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        center_loc = grid_loc * self.trash_size + self.trash_size // 2\n",
    "        # add to the references\n",
    "        self.trash_grid_to_center[tuple(grid_loc)] = center_loc\n",
    "        self.trash_center_to_grid[tuple(center_loc)] = grid_loc\n",
    "        self.trash_center_to_source[tuple(center_loc)] = agent_id\n",
    "        # add to the trash grid\n",
    "        self.trash_grid[grid_loc[0], grid_loc[1]] = 1\n",
    "        # add to the trash map\n",
    "        y, x = tuple(center_loc)\n",
    "        c = (self.trash_size // 2)\n",
    "        self.trash_map.paste(\n",
    "            self.icons['trash'], (x - c, y - c)\n",
    "        )\n",
    "\n",
    "    def sample_trash(self, agent_id: str) -> bool:\n",
    "        \"\"\"Sample trash\n",
    "\n",
    "        Sample trash randomly at a given agents' segment\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            bool: True if sampled successfully and False otherwise\n",
    "        \"\"\"\n",
    "        a_reg = np.logical_and(\n",
    "            self.trash_segment_map[agent_id],\n",
    "            np.logical_and(\n",
    "                np.logical_not(self.non_trash_grid),\n",
    "                np.logical_not(self.trash_grid)\n",
    "            )\n",
    "        ).astype(np.uint8)\n",
    "        a_loc = np.argwhere(a_reg > 0)\n",
    "        if a_loc.shape[0] > 0:\n",
    "            idx = self.rng.integers(low=0, high=a_loc.shape[0])\n",
    "            grid_loc = a_loc[idx]\n",
    "            self.add_trash(grid_loc=grid_loc, agent_id=agent_id)\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        return done\n",
    "\n",
    "    def sample_resource(self, agent_id: str) -> bool:\n",
    "        \"\"\"Sample resource\n",
    "\n",
    "        Sample resource randomly at a given agents' segment\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            bool: True if sampled successfully and False otherwise\n",
    "        \"\"\"\n",
    "        a_reg = np.logical_and(\n",
    "            self.resource_segment_map[agent_id],\n",
    "            np.logical_and(\n",
    "                np.logical_not(self.non_resource_grid),\n",
    "                np.logical_not(self.resource_grid)\n",
    "            )\n",
    "        ).astype(np.uint8)\n",
    "        a_loc = np.argwhere(a_reg > 0)\n",
    "        if a_loc.shape[0] > 0:\n",
    "            idx = self.rng.integers(low=0, high=a_loc.shape[0])\n",
    "            grid_loc = a_loc[idx]\n",
    "            self.add_resource(grid_loc=grid_loc)\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        return done\n",
    "\n",
    "    def move_candidate(\n",
    "        self, action_id: int,\n",
    "        agent_id: str, step_size: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "        \"\"\"Movement candidate\n",
    "\n",
    "        Propose movement candidate without taking into an account\n",
    "        unreachable regions\n",
    "\n",
    "        Args:\n",
    "            action_id: movement action integer id\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "            step_size: agent step size in pixels\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, float]: tuple which contains:\n",
    "                - initial location\n",
    "                - new candidate location\n",
    "                - new direction\n",
    "        \"\"\"\n",
    "        loc = self.agents_state[agent_id]['loc']\n",
    "        dir = self.agents_state[agent_id]['dir_pi']\n",
    "        if action_id == 0:\n",
    "            new_dir = dir\n",
    "        elif action_id == 1:\n",
    "            new_dir = (dir + 0.5) % 2.\n",
    "        elif action_id == 2:\n",
    "            new_dir = (dir - 0.5) % 2.\n",
    "        elif action_id == 3:\n",
    "            new_dir = (dir - 1.) % 2.\n",
    "        else:\n",
    "            new_dir = None\n",
    "            raise ValueError(\n",
    "                f'Invalid action_id for movement: {action_id}'\n",
    "            )\n",
    "        shift = dir_to_shift(dir=new_dir)\n",
    "        new_loc = loc + shift * step_size\n",
    "        new_loc = np.clip(\n",
    "            new_loc,\n",
    "            a_min=(0 + self.agent_size // 2),\n",
    "            a_max=(self.grid_size - 1 - self.agent_size // 2)\n",
    "        ).astype(int)\n",
    "        return loc, new_loc, new_dir\n",
    "\n",
    "    def move(self, action_id: int, agent_id: str) -> None:\n",
    "        \"\"\"Make movement\n",
    "\n",
    "        Make agent movement with respect to unreachable regions\n",
    "\n",
    "        Args:\n",
    "            action_id: movement action integer id\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        valid = False\n",
    "        new_loc, new_dir = None, None\n",
    "        step = self.move_step\n",
    "        while not valid and step > -1:\n",
    "            init_loc, new_loc, new_dir = self.move_candidate(\n",
    "                action_id=action_id, agent_id=agent_id,\n",
    "                step_size=step\n",
    "            )\n",
    "            valid = not bool(self.non_reachable[new_loc[0], new_loc[1]])\n",
    "            step -= 1\n",
    "        self.agents_state[agent_id]['loc'] = new_loc\n",
    "        self.agents_state[agent_id]['dir_pi'] = new_dir\n",
    "\n",
    "    def pickup(\n",
    "        self, agent_id: str, type: str\n",
    "    ) -> Tuple[np.ndarray, bool]:\n",
    "        \"\"\"Pickup item\n",
    "\n",
    "        pickup item from game field\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "            type: one of {'resource', 'trash'}\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, bool]: picked item loc and pickup\n",
    "                status boolean\n",
    "        \"\"\"\n",
    "        status, item_loc = False, None\n",
    "        loc = self.agents_state[agent_id]['loc']\n",
    "        reach = self.agent_reach\n",
    "        if type == 'resource':\n",
    "            ridxs = np.array(list(self.resource_grid_to_center.values()))\n",
    "        elif type == 'trash':\n",
    "            ridxs = np.array(list(self.trash_grid_to_center.values()))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Invalid item to pickup: {type}'\n",
    "            )\n",
    "        if ridxs.shape[0] > 0:\n",
    "            deltas = np.abs(ridxs - loc)\n",
    "            max_dist = deltas.max(axis=1)\n",
    "            valid = np.argwhere(max_dist < reach).squeeze(axis=-1)\n",
    "            if len(valid) > 0:\n",
    "                if len(valid) > 1:\n",
    "                    mean_dist = deltas[valid].mean(axis=-1)\n",
    "                    min_dist = np.argmin(mean_dist)\n",
    "                    valid = valid[min_dist]\n",
    "                else:\n",
    "                    valid = valid[0]\n",
    "                item_loc = ridxs[valid]\n",
    "                if not self.agents_state[agent_id]['inventory'][type]:\n",
    "                    if type == 'resource':\n",
    "                        self.delete_resource(center_loc=tuple(item_loc))\n",
    "                    else:\n",
    "                        trash_source = self.delete_trash(center_loc=tuple(item_loc))\n",
    "                        self.agents_state[agent_id]['inventory']['trash_source'] = trash_source\n",
    "                    self.agents_state[agent_id]['inventory'][type] = True\n",
    "                    status = True\n",
    "        return item_loc, status\n",
    "\n",
    "    def throw_resource(self, agent_id: str) -> bool:\n",
    "        \"\"\"Throw resource\n",
    "\n",
    "        Throw resource onto the game field at the current\n",
    "        agent location if possible\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            bool: True if succeeded, False otherwise\n",
    "        \"\"\"\n",
    "        ac_loc = self.agents_state[agent_id]['loc']\n",
    "        ag_loc = ac_loc // self.resource_size\n",
    "        done = False\n",
    "        if self.agents_state[agent_id]['inventory']['resource']:\n",
    "            # check if we can throw it here\n",
    "            invalid = np.logical_or(\n",
    "                self.non_resource_grid,\n",
    "                self.resource_grid\n",
    "            )\n",
    "            valid = not bool(invalid[ag_loc[0], ag_loc[1]])\n",
    "            if valid:\n",
    "                self.add_resource(grid_loc=ag_loc)\n",
    "                # remove from inventory\n",
    "                self.agents_state[agent_id]['inventory']['resource'] = False\n",
    "                done = True\n",
    "        return done\n",
    "\n",
    "    def throw_trash(self, agent_id: str) -> bool:\n",
    "        \"\"\"Throw trash\n",
    "\n",
    "        Throw trash onto the game field at the current\n",
    "        agent location if possible\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            bool: True if succeeded, False otherwise\n",
    "        \"\"\"\n",
    "        ac_loc = self.agents_state[agent_id]['loc']\n",
    "        ag_loc = ac_loc // self.trash_size\n",
    "        done = False\n",
    "        if self.agents_state[agent_id]['inventory']['trash']:\n",
    "            # check if we can throw it here\n",
    "            invalid = np.logical_or(\n",
    "                self.non_trash_grid,\n",
    "                self.trash_grid\n",
    "            )\n",
    "            valid = not bool(invalid[ag_loc[0], ag_loc[1]])\n",
    "            if valid:\n",
    "                trash_source = self.agents_state[agent_id]['inventory']['trash_source']\n",
    "                self.add_trash(grid_loc=ag_loc, agent_id=trash_source)\n",
    "                # remove from inventory\n",
    "                self.agents_state[agent_id]['inventory']['trash'] = False\n",
    "                self.agents_state[agent_id]['inventory']['trash_source'] = None\n",
    "                done = True\n",
    "        return done\n",
    "\n",
    "    def recycle_resource(self, agent_id: str) -> bool:\n",
    "        \"\"\"Recycle resource\n",
    "\n",
    "        Recycle resource at the fabricator if:\n",
    "            1) It is in the inventory\n",
    "            2) Fabricator is close enough\n",
    "            3) Agent is not blocked\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            bool: True if succeeded, False otherwise\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        if not self.blocked[agent_id]:\n",
    "            if self.agents_state[agent_id]['inventory']['resource']:\n",
    "                ac_loc = self.agents_state[agent_id]['loc']\n",
    "                machine_loc = self.machines[agent_id]['center']\n",
    "                delta = np.abs(machine_loc - ac_loc).max()\n",
    "                if delta <= self.machine_reach:\n",
    "                    self.agents_state[agent_id]['inventory']['resource'] = False\n",
    "                    self.agents_state[agent_id]['wealth'] += self.resource_price\n",
    "                    self.sample_trash(agent_id=agent_id)\n",
    "                    done = True\n",
    "        return done\n",
    "\n",
    "    def recycle_trash(self, agent_id: str) -> bool:\n",
    "        \"\"\"Recycle trash\n",
    "\n",
    "        Recycle trash at the recycler if:\n",
    "            1) It is in the inventory\n",
    "            2) Recycler is close enough\n",
    "            3) Agent is not blocked\n",
    "            4) Agent has enough money for recycling\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            bool: True if succeeded, False otherwise\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        if not self.blocked[agent_id]:\n",
    "            has_trash = self.agents_state[agent_id]['inventory']['trash']\n",
    "            has_money = self.agents_state[agent_id]['wealth'] >= self.recycle_cost\n",
    "            if has_trash and has_money:\n",
    "                ac_loc = self.agents_state[agent_id]['loc']\n",
    "                recycler_loc = self.recyclers[agent_id]['center']\n",
    "                delta = np.abs(recycler_loc - ac_loc).max()\n",
    "                if delta <= self.machine_reach:\n",
    "                    self.agents_state[agent_id]['inventory']['trash'] = False\n",
    "                    self.agents_state[agent_id]['inventory']['trash_source'] = None\n",
    "                    self.agents_state[agent_id]['wealth'] -= self.recycle_cost\n",
    "                    done = True\n",
    "        return done\n",
    "\n",
    "    def drop_resource(self, agent_id: str) -> str:\n",
    "        \"\"\"Drop resource\n",
    "\n",
    "        Throw or recycle resource. If recycle is possible,\n",
    "        recycle it at the fabricator, otherwise drop onto\n",
    "        the game field\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            str: drop status string. One in\n",
    "                {'dropped', 'drop_failed', 'resource_recycled'}\n",
    "        \"\"\"\n",
    "        recycle_done = self.recycle_resource(agent_id=agent_id)\n",
    "        if not recycle_done:\n",
    "            throw_done = self.throw_resource(agent_id=agent_id)\n",
    "            if throw_done:\n",
    "                status = 'dropped'\n",
    "            else:\n",
    "                status = 'drop_failed'\n",
    "        else:\n",
    "            status = 'resource_recycled'\n",
    "        return status\n",
    "\n",
    "    def drop_trash(self, agent_id: str) -> str:\n",
    "        \"\"\"Drop trash\n",
    "\n",
    "        Throw or recycle trash. If recycle is possible,\n",
    "        recycle it at the recycler, otherwise drop onto\n",
    "        the game field\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            str: drop status string. One in\n",
    "                {'dropped', 'drop_failed', 'resource_recycled'}\n",
    "        \"\"\"\n",
    "        recycle_done = self.recycle_trash(agent_id=agent_id)\n",
    "        if not recycle_done:\n",
    "            throw_done = self.throw_trash(agent_id=agent_id)\n",
    "            if throw_done:\n",
    "                status = 'dropped'\n",
    "            else:\n",
    "                status = 'drop_failed'\n",
    "        else:\n",
    "            status = 'trash_recycled'\n",
    "        return status\n",
    "\n",
    "    def local_view(self, state_pad: np.ndarray, agent_id: str) -> np.ndarray:\n",
    "        \"\"\"Render local view\n",
    "\n",
    "        Render local view image array for a given agent ID\n",
    "\n",
    "        Args:\n",
    "            state_pad: padded engine state image\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: local view image array\n",
    "        \"\"\"\n",
    "        agent_loc = self.agents_state[agent_id]['loc']\n",
    "        agent_dir = self.agents_state[agent_id]['dir_pi']\n",
    "        p = self.obs_dim\n",
    "        if agent_dir == 0.0:\n",
    "            x_high = agent_loc[0] + p // 2  # right\n",
    "            x_low = x_high - p  # left\n",
    "            y_low = agent_loc[1] - self.agent_size // 2 + 1  # back\n",
    "            y_high = y_low + p  # forward\n",
    "        elif agent_dir == 0.5:\n",
    "            x_high = agent_loc[0] + self.agent_size // 2  # back\n",
    "            x_low = x_high - p  # forward\n",
    "            y_high = agent_loc[1] + p // 2  # right\n",
    "            y_low = y_high - p  # left\n",
    "        elif agent_dir == 1.0:\n",
    "            x_low = agent_loc[0] - p // 2 + 1  # right\n",
    "            x_high = x_low + p  # left\n",
    "            y_high = agent_loc[1] + self.agent_size // 2  # back\n",
    "            y_low = y_high - p  # forward\n",
    "        elif agent_dir == 1.5:\n",
    "            x_low = agent_loc[0] - self.agent_size // 2 + 1  # back\n",
    "            x_high = x_low + p  # forward\n",
    "            y_low = agent_loc[1] - p // 2 + 1  # right\n",
    "            y_high = y_low + p  # left\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Invalid agent direction: {agent_dir}'\n",
    "            )\n",
    "        xl, xh, yl, yh = x_low + p, x_high + p, y_low + p, y_high + p\n",
    "        obs = state_pad[xl:xh, yl:yh, :]\n",
    "        all_machines = self.all_machines[xl:xh, yl:yh, :]\n",
    "        perspective = self.agents_perspectives[agent_id][xl:xh, yl:yh, :]\n",
    "        obs = obs * np.logical_not(all_machines)\n",
    "        obs = obs + perspective\n",
    "        if agent_dir == 0.0:\n",
    "            obs = np.rot90(obs, k=1)\n",
    "        elif agent_dir == 0.5:\n",
    "            pass\n",
    "        elif agent_dir == 1.0:\n",
    "            obs = np.rot90(obs, k=-1)\n",
    "        elif agent_dir == 1.5:\n",
    "            obs = np.rot90(obs, k=2)\n",
    "        else:\n",
    "            pass\n",
    "        return obs.copy()\n",
    "\n",
    "    def local_proprio(self, agent_id: str) -> np.ndarray:\n",
    "        \"\"\"Get local proprioceptive obs\n",
    "\n",
    "        Get local proprioceptive obs for a given agent ID\n",
    "\n",
    "        Args:\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: subjective proprioceptive obs\n",
    "        \"\"\"\n",
    "        money = self.agents_state[agent_id]['wealth'] / self.resource_price\n",
    "        dir = self.agents_state[agent_id]['dir_pi']\n",
    "        loc = self.agents_state[agent_id]['loc']\n",
    "        center = self.centers[agent_id]\n",
    "\n",
    "        # North\n",
    "        if dir == 0.0:\n",
    "            x, y = 1, 0\n",
    "        elif dir == 0.5:\n",
    "            x, y = 0, 1\n",
    "        elif dir == 1.0:\n",
    "            x, y = -1, 0\n",
    "        elif dir == 1.5:\n",
    "            x, y = 0, -1\n",
    "        else:\n",
    "            x, y = 0, 0\n",
    "\n",
    "        center_vec = loc_to_coord(center, self.grid_size) - \\\n",
    "            loc_to_coord(loc, self.grid_size)\n",
    "        rho, phi = cart2pol(center_vec[0].item(), center_vec[1].item())\n",
    "        rho, phi = rho / self.grid_size, phi / np.pi\n",
    "        is_home = float(self.is_home_segment(agent_id=agent_id))\n",
    "        has_resource = float(\n",
    "            self.agents_state[agent_id]['inventory']['resource'])\n",
    "        has_trash = float(self.agents_state[agent_id]['inventory']['trash'])\n",
    "        _, north_dir = cart2pol(x, y)\n",
    "        north_dir = north_dir / np.pi\n",
    "        proprio = np.array([\n",
    "            money, has_resource, has_trash,\n",
    "            rho, phi, north_dir, is_home\n",
    "        ])\n",
    "        return proprio.astype(np.float32)\n",
    "\n",
    "    def local_obs(\n",
    "        self, state_pad: np.ndarray, agent_id: str\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Get local observation\n",
    "\n",
    "        Get local composite observation for a given agent ID\n",
    "\n",
    "        Args:\n",
    "            state_pad: padded engine state image\n",
    "            agent_id: agent ID in form `agent_{i}`\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, np.ndarray]: composite observation with the\n",
    "                following key-value pairs:\n",
    "                    - 'image': local visual observation\n",
    "                    - 'proprio': subjective proprioceptive observation\n",
    "        \"\"\"\n",
    "        view = self.local_view(state_pad=state_pad, agent_id=agent_id)\n",
    "        proprio = self.local_proprio(agent_id=agent_id)\n",
    "        return {'image': view, 'proprio': proprio}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8b1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents\n",
    "\n",
    "import abc\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BaseAgent(metaclass=abc.ABCMeta):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def load(self, ckpt_dir: str) -> None:\n",
    "        \"\"\"Agent Loading\n",
    "\n",
    "        Loading an agent from the directory with artifacts.\n",
    "\n",
    "        Args:\n",
    "            ckpt_dir: path to an individual directory with artifacts\n",
    "                and the `agent_config.yaml` file for this agent\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_action(self, observation: Dict[str, np.ndarray]) -> int:\n",
    "        \"\"\"Getting action\n",
    "\n",
    "        Getting action from the agent based on visual observation\n",
    "\n",
    "        Args:\n",
    "            observation: dictionary with the following keys and values\n",
    "                \"image\": numpy array with an image of a local field of\n",
    "                    view with dimensions (60, 60, 3) and np.uint8 data type\n",
    "                \"proprio\": numpy array with proprioceptive information\n",
    "                    about the position of the agent on the general map\n",
    "                    and the condition of its inventory with dimensions (7,)\n",
    "                    and np.float32 data type\n",
    "        Returns:\n",
    "            int: index of the selected action\n",
    "                {0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reset_state(self) -> None:\n",
    "        \"\"\"Resetting the internal state\n",
    "\n",
    "        In case of accumulation of internal context for decision-making\n",
    "        during the episode, this method is called before each new\n",
    "        simulation to clear the internal state before a new episode.\n",
    "        If the internal context is not used, the method can be left in\n",
    "        its current form.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class RandomAgent(BaseAgent):\n",
    "    \"\"\"Random agent\n",
    "\n",
    "    Random agent for AIJ Multi-agent AI Contest\n",
    "\n",
    "    Attributes:\n",
    "        action_dim: discrete action dimension, for this contest\n",
    "            is always 9\n",
    "        rng: numpy random number generator for reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_dim: int = 9,\n",
    "        seed: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"Initialise random agent\n",
    "\n",
    "        Args:\n",
    "            action_dim: discrete action dimension, for this contest\n",
    "                is always 9\n",
    "            seed: random number generator seed\n",
    "        \"\"\"\n",
    "        self.action_dim = action_dim\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0, int(1e6), 1).item()\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def load(self, ckpt_dir: str) -> None:\n",
    "        \"\"\"Agent Loading\n",
    "\n",
    "        Loading an agent from the directory with artifacts.\n",
    "\n",
    "        Args:\n",
    "            ckpt_dir: path to an individual directory with artifacts\n",
    "                and the `agent_config.yaml` file for this agent\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_action(self, observation: Dict[str, np.ndarray]) -> int:\n",
    "        \"\"\"Getting action\n",
    "\n",
    "        Getting action from random agent\n",
    "\n",
    "        Args:\n",
    "            observation: dictionary with the following keys and values\n",
    "                \"image\": numpy array with an image of a local field of\n",
    "                    view with dimensions (60, 60, 3) and np.uint8 data type\n",
    "                \"proprio\": numpy array with proprioceptive information\n",
    "                    about the position of the agent on the general map\n",
    "                    and the condition of its inventory with dimensions (7,)\n",
    "                    and np.float32 data type\n",
    "        Returns:\n",
    "            int: index of the selected action\n",
    "                {0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
    "        \"\"\"\n",
    "        return self.rng.integers(0, self.action_dim, 1).item()\n",
    "\n",
    "    def reset_state(self) -> None:\n",
    "        \"\"\"Resetting the internal state\n",
    "\n",
    "        In case of accumulation of internal context for decision-making\n",
    "        during the episode, this method is called before each new\n",
    "        simulation to clear the internal state before a new episode.\n",
    "        If the internal context is not used, the method can be left in\n",
    "        its current form.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f37a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "import functools\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "# !pip3 install pettingzoo\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.spaces import Dict as DictSpace\n",
    "from gymnasium.spaces import Discrete\n",
    "from pettingzoo import ParallelEnv\n",
    "from skimage.transform import resize\n",
    "\n",
    "TimestampType = Tuple[\n",
    "    Dict[str, Dict[str, np.ndarray]], Dict[str, float],\n",
    "    Dict[str, bool], Dict[str, bool], Dict[str, dict]\n",
    "]\n",
    "\n",
    "MOVES = [\"FORWARD\", \"LEFT\", \"RIGHT\", \"BACKWARD\",\n",
    "         \"PICKUP_RESOURCE\", \"PICKUP_TRASH\",\n",
    "         \"DROP_RESOURCE\", \"DROP_TRASH\", \"NOOP\"]\n",
    "\n",
    "\n",
    "class AijMultiagentEnv(ParallelEnv):\n",
    "    \"\"\"Parallel multi-agent environment for AIJ\n",
    "    Multi-Agent RL contest\n",
    "\n",
    "    Environment at the testing system will have the same\n",
    "    hyperparameter setting as below, so it is not recommended\n",
    "    to change it\n",
    "\n",
    "    Attributes:\n",
    "        grid_size: 2D square game field size\n",
    "        obs_dim: local visual observation size\n",
    "        move_step: movement step size in pixels\n",
    "        resource_price: reward given for resource processing\n",
    "        recycle_cost: cost of recycling trash\n",
    "        border_distort_range: noise range for borders distortion\n",
    "        max_edge_dev: maximum segment border shift\n",
    "        max_tries: maximum attempts for border generation\n",
    "        machine_size: machine icon size in pixels\n",
    "        machine_reach: size of machine interaction region\n",
    "        agent_size: agent icon size in pixels\n",
    "        agent_reach: agent reach when picking up items\n",
    "        resource_size: resource icon size in pixels\n",
    "        trash_size: trash icon size in pixels\n",
    "        resource_prob: probability to spawn resource at a given point\n",
    "        border_display_width: segments borders thickness\n",
    "        ecology_penalty: decrease in agent's ecology score caused by 1 trash item\n",
    "        neighbour_ecology_weight: neighbour ecology effect at the resource\n",
    "            respawn rate\n",
    "        global_ecology_weight: global ecology effect at the resource\n",
    "            respawn rate\n",
    "        init_respawn_prob: initial probability to spawn resource at a given point\n",
    "        blocked_vanish_alpha: blocked segment fogging degree\n",
    "        max_dead_segments: max number of blocked segments before global\n",
    "            termination\n",
    "    \"\"\"\n",
    "    # Hardcoded hyperparameters\n",
    "    grid_size: int = 210  # should be div by resource_size and trash_size\n",
    "    obs_dim: int = 60  # should be divisible by 5\n",
    "    move_step: int = 7\n",
    "    resource_price: int = 10\n",
    "    recycle_cost: int = 4\n",
    "    border_distort_range: Tuple[int, int] = (-1, 2)\n",
    "    max_edge_dev: float = 0.1\n",
    "    max_tries: int = 25\n",
    "    machine_size: int = 9\n",
    "    machine_reach: int = 9\n",
    "    agent_size: int = 9\n",
    "    agent_reach: int = 9\n",
    "    resource_size: int = 7\n",
    "    trash_size: int = 5\n",
    "    resource_prob: float = 0.075\n",
    "    border_display_width: int = 2\n",
    "    ecology_penalty: int = 20\n",
    "    neighbour_ecology_weight: float = 0.2\n",
    "    global_ecology_weight: float = 0.3\n",
    "    init_respawn_prob: float = 0.015\n",
    "    blocked_vanish_alpha: float = 0.25\n",
    "    max_dead_segments: int = 4\n",
    "\n",
    "    # To enable rendering\n",
    "    metadata = {\"render_modes\": ['rgb_array'],\n",
    "                \"name\": \"aij_multiagent_env\"}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_cycles: Optional[int] = 1000,\n",
    "        state_size: Optional[int] = 110,\n",
    "        render_mode: Optional[str] = 'rgb_array',\n",
    "    ):\n",
    "        \"\"\"Multi-agent RL Environment\n",
    "\n",
    "        Multi-agent RL Environment for AIJ Contest 2024\n",
    "\n",
    "        Args:\n",
    "            max_cycles: maximum simulation length in time steps\n",
    "            state_size: display state size for rendering\n",
    "            render_mode: render mode\n",
    "\n",
    "        Valid Action Space:\n",
    "            0: move forward by `move_step` pixels if possible\n",
    "            1: move left by `move_step` pixels if possible\n",
    "            2: move right by `move_step` pixels if possible\n",
    "            3: move backward by `move_step` pixels if possible\n",
    "            4: pickup resource (if closer than `agent_reach` pixels)\n",
    "            5: pickup trash (if closer than `agent_reach` pixels)\n",
    "            6: throw resource (put into machine if closer than `machine_reach`)\n",
    "            7: throw trash (put into recycler if closer than `machine_reach`)\n",
    "            8: noop\n",
    "        \"\"\"\n",
    "        self.engine = None\n",
    "        self.rng = None\n",
    "        self.ecology_scores = None\n",
    "        self.num_moves = None\n",
    "        self.current_state = None\n",
    "        self.seed = None\n",
    "        self.render_mode = render_mode\n",
    "        self.max_cycles = max_cycles\n",
    "        self.state_size = state_size\n",
    "        self.action_meanings = MOVES\n",
    "        self.possible_agents = [f'agent_{i}' for i in range(8)]\n",
    "        self.neighbours_mapping = {\n",
    "            'agent_0': ['agent_1', 'agent_3'],\n",
    "            'agent_1': ['agent_0', 'agent_2'],\n",
    "            'agent_2': ['agent_1', 'agent_4'],\n",
    "            'agent_3': ['agent_0', 'agent_5'],\n",
    "            'agent_4': ['agent_2', 'agent_7'],\n",
    "            'agent_5': ['agent_3', 'agent_6'],\n",
    "            'agent_6': ['agent_5', 'agent_7'],\n",
    "            'agent_7': ['agent_4', 'agent_6'],\n",
    "        }\n",
    "        self.agents = self.possible_agents.copy()\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _get_engine(\n",
    "        cls, seed: Optional[int] = None\n",
    "    ) -> Tuple[GameEngine, np.random.Generator, int]:\n",
    "        \"\"\"Get engine\n",
    "\n",
    "        Get game engine for simulation round\n",
    "\n",
    "        Args:\n",
    "            seed: random seed for simulation\n",
    "\n",
    "        Returns:\n",
    "            Tuple[GameEngine, np.random.Generator, int]: tuple of:\n",
    "                - GameEngine instance\n",
    "                - numpy random number generator\n",
    "                - seed for logging\n",
    "        \"\"\"\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0, int(1e6), 1).item()\n",
    "        engine = GameEngine(\n",
    "            seed=seed,\n",
    "            grid_size=cls.grid_size,\n",
    "            obs_dim=cls.obs_dim,\n",
    "            move_step=cls.move_step,\n",
    "            resource_price=cls.resource_price,\n",
    "            recycle_cost=cls.recycle_cost,\n",
    "            border_distort_range=cls.border_distort_range,\n",
    "            max_edge_dev=cls.max_edge_dev,\n",
    "            max_tries=cls.max_tries,\n",
    "            machine_size=cls.machine_size,\n",
    "            machine_reach=cls.machine_reach,\n",
    "            agent_size=cls.agent_size,\n",
    "            agent_reach=cls.agent_reach,\n",
    "            resource_size=cls.resource_size,\n",
    "            trash_size=cls.trash_size,\n",
    "            resource_prob=cls.resource_prob,\n",
    "            border_display_width=cls.border_display_width,\n",
    "            blocked_vanish_alpha=cls.blocked_vanish_alpha\n",
    "        )\n",
    "        rng = np.random.default_rng(seed + 1)\n",
    "        return engine, rng, seed\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent) -> DictSpace:\n",
    "        \"\"\"Get observation space\n",
    "\n",
    "        Get environment observation space\n",
    "\n",
    "        Args:\n",
    "            agent: agent to return observation space for\n",
    "\n",
    "        Returns:\n",
    "            DictSpace: specification of composite observation\n",
    "        \"\"\"\n",
    "        d = self.engine.obs_dim\n",
    "        gs, diag = self.engine.grid_size, self.engine.diag\n",
    "        image_space = Box(0, 255, (d, d, 3), np.uint8, seed=self.rng)\n",
    "        low = np.array([0., 0., 0., 0., -1., -1., 0.])\n",
    "        high = np.array([np.inf, 1., 1., diag / gs, 1., 1., 1.])\n",
    "        proprio_space = Box(\n",
    "            low=low, high=high, shape=(7,), dtype=np.float32, seed=self.rng)\n",
    "        return DictSpace({'image': image_space, 'proprio': proprio_space})\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent) -> Discrete:\n",
    "        \"\"\"Get action space\n",
    "\n",
    "        Get environment action space\n",
    "\n",
    "        Args:\n",
    "            agent: agent to return action space for\n",
    "\n",
    "        Returns:\n",
    "            Discrete: specification of discrete action space\n",
    "        \"\"\"\n",
    "        return Discrete(len(self.action_meanings), seed=self.rng)\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, options: Optional[Any] = None\n",
    "    ) -> Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict]]:\n",
    "        \"\"\"Reset environment\n",
    "\n",
    "        Reset environment to its initial state\n",
    "\n",
    "        Args:\n",
    "            seed: numpy random seed\n",
    "            options: any other additional options\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict]]:\n",
    "                tuple which contains:\n",
    "                    - composite observations for each agent\n",
    "                    - infos for each agent\n",
    "        \"\"\"\n",
    "        self.num_moves = 0\n",
    "        self.agents = self.possible_agents.copy()\n",
    "        self.engine, self.rng, self.seed = self._get_engine(seed=seed)\n",
    "        self.ecology_scores = {a: 100 for a in self.agents}\n",
    "        # Render next observations\n",
    "        state = self.engine.get_state()\n",
    "        self.current_state = state\n",
    "        # Local perspective state\n",
    "        local_agents_map = self.engine.agents_map(local_mode=True)\n",
    "        local_state = state * np.logical_not(\n",
    "            local_agents_map > 0)\n",
    "        local_state = local_state + local_agents_map\n",
    "        # Render local observations\n",
    "        state_pad = self.engine.pad_state(local_state)\n",
    "        obs = {a: self.engine.local_obs(\n",
    "            state_pad=state_pad, agent_id=a) for a in self.agents}\n",
    "        # Log information\n",
    "        infos = {a: {\n",
    "            'ecology_score': self.ecology_scores[a],\n",
    "            'num_trash': self.engine.trash_by_segment(agent_id=a),\n",
    "            'num_resource': self.engine.resource_by_segment(agent_id=a),\n",
    "            'dead_ecology': self.engine.blocked[a]\n",
    "        } for a in self.agents}\n",
    "        return obs, infos\n",
    "\n",
    "    def _make_action(self, agent_id: str, action_id: int) -> None:\n",
    "        \"\"\"Make action\n",
    "\n",
    "        Execute action in the environment for a given agent\n",
    "\n",
    "        Args:\n",
    "             agent_id: agent ID in form `agent_{i}`\n",
    "             action_id: action integer id from valid actions set\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if action ID is outside valid action set\n",
    "        \"\"\"\n",
    "        if 4 > action_id >= 0:\n",
    "            self.engine.move(agent_id=agent_id, action_id=action_id)\n",
    "        elif action_id == 4:\n",
    "            self.engine.pickup(agent_id=agent_id, type='resource')\n",
    "        elif action_id == 5:\n",
    "            self.engine.pickup(agent_id=agent_id, type='trash')\n",
    "        elif action_id == 6:\n",
    "            self.engine.drop_resource(agent_id=agent_id)\n",
    "        elif action_id == 7:\n",
    "            self.engine.drop_trash(agent_id=agent_id)\n",
    "        elif action_id == 8:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Invalid action: {action_id} for agent: {agent_id}')\n",
    "\n",
    "    def _update_ecology_scores(self) -> Dict[str, int]:\n",
    "        \"\"\"Update ecology scores\n",
    "\n",
    "        Update ecology scores given the most recent trash\n",
    "        distribution information\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, int]: current number of trash per segment\n",
    "        \"\"\"\n",
    "        new_ec_scores = {}\n",
    "        trash_by_agent = {}\n",
    "        for a, s in self.ecology_scores.items():\n",
    "            n_trash = self.engine.trash_by_segment(agent_id=a)\n",
    "            trash_by_agent[a] = n_trash\n",
    "            ec_score = max(0, 100 - n_trash * self.ecology_penalty)\n",
    "            new_ec_scores[a] = ec_score * int(not self.engine.blocked[a])\n",
    "        self.ecology_scores = new_ec_scores\n",
    "        return trash_by_agent\n",
    "\n",
    "    def _get_resource_respawn_probs(self) -> Dict[str, float]:\n",
    "        \"\"\"Get resource respawn probs\n",
    "\n",
    "        Get resource respawn probability given current trash\n",
    "        distribution\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, int]: respawn probs by agent ID\n",
    "        \"\"\"\n",
    "        resp_probs = {}\n",
    "        gs = np.mean(list(self.ecology_scores.values())).item()\n",
    "        for a, s in self.ecology_scores.items():\n",
    "            n1, n2 = self.neighbours_mapping[a]\n",
    "            ns1, ns2 = self.ecology_scores[n1], self.ecology_scores[n2]\n",
    "            mean_ns = (ns1 + ns2) / 2\n",
    "            nw, p = self.neighbour_ecology_weight, self.init_respawn_prob\n",
    "            gw = self.global_ecology_weight\n",
    "            lw = max(0., 1 - nw - gw)\n",
    "            r = ((lw * s + gw * gs + nw * mean_ns) / 100)\n",
    "            resp_probs[a] = (r ** 2.15) * p * int(not self.engine.blocked[a])\n",
    "        return resp_probs\n",
    "\n",
    "    def _get_terminations(self) -> Dict[str, bool]:\n",
    "        \"\"\"Get terminations\n",
    "\n",
    "        Get global termination and impose blocks according to\n",
    "        local ecology scores\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, bool]: global termination by agent ID\n",
    "                (all True or all False)\n",
    "        \"\"\"\n",
    "        global_termination = False\n",
    "        for a, s in self.ecology_scores.items():\n",
    "            terminated = s == 0\n",
    "            if terminated:\n",
    "                self.engine.add_block(agent_id=a)\n",
    "        n_blocked = sum(list(self.engine.blocked.values()))\n",
    "        if n_blocked > self.max_dead_segments:\n",
    "            global_termination = True\n",
    "        return {a: global_termination for a in self.possible_agents}\n",
    "\n",
    "    def state(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Get global state\n",
    "\n",
    "        Get global state for CTDE multi-agent RL paradigm.\n",
    "        Note! That method won't be called at testing system\n",
    "        and may be used only for training agents.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, np.ndarray]: global state with following key-value\n",
    "                pairs:\n",
    "                    - 'image': global visual state, shape:\n",
    "                        (self.state_size, self.state_size, 3)\n",
    "                    - 'wealth': array with agents wealth, shape: (8,)\n",
    "                    - 'has_resource': array with binary flag, indicating that\n",
    "                        resource is in inventory, shape: (8,)\n",
    "                    - 'has_resource': array with binary flag, indicating that\n",
    "                        trash is in inventory, shape: (8,)\n",
    "        \"\"\"\n",
    "        state = {}\n",
    "        image = self.current_state.copy()\n",
    "        image = resize(image, (self.state_size, self.state_size))\n",
    "        state['image'] = np.round(image * 255, 0).astype(np.uint8)\n",
    "        state['wealth'] = np.array(\n",
    "            [self.engine.agents_state[a]['wealth']\n",
    "             for a in self.possible_agents])\n",
    "        state['has_resource'] = np.array(\n",
    "            [self.engine.agents_state[a]['inventory']['resource']\n",
    "             for a in self.possible_agents]).astype(int)\n",
    "        state['has_trash'] = np.array(\n",
    "            [self.engine.agents_state[a]['inventory']['trash']\n",
    "             for a in self.possible_agents]).astype(int)\n",
    "        return state\n",
    "\n",
    "    def render(self) -> np.ndarray:\n",
    "        \"\"\"Render global state\n",
    "\n",
    "        Render global state as numpy image array\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: global visual state, shape:\n",
    "                (self.state_size, self.state_size, 3)\n",
    "        \"\"\"\n",
    "        return self.state()['image']\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Close all rendering windows\"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, actions: Dict[str, int]) -> TimestampType:\n",
    "        \"\"\"Perform simulation step\n",
    "\n",
    "        Perform simulation step in parallel multi-agent RL style.\n",
    "        In case of conflicting actions (for example two agents\n",
    "        picking up the same resource, priorities are assigned\n",
    "        randomly). Note: both terminations and truncations occur\n",
    "        simultaneously for all agents participating in the\n",
    "        simulation.\n",
    "\n",
    "        Args:\n",
    "            actions: dictionary with action IDs for each agent\n",
    "        Returns:\n",
    "            TimestampType: environment time stamp as a tuple of:\n",
    "                - Dict[str, Dict[str, np.ndarray]]: composite observations\n",
    "                    for each agent\n",
    "                - Dict[str, float]: rewards for each agent\n",
    "                - Dict[str, bool]: terminations for each agent\n",
    "                - Dict[str, bool]: truncations for each agent\n",
    "                - Dict[str, dict]: infos for each agent\"\"\"\n",
    "        self.num_moves += 1\n",
    "        # Cache money to calculate rewards\n",
    "        old_w = {a: self.engine.agents_state[a]['wealth'] for a in self.agents}\n",
    "        # Apply actions in random order (to reconcile possible conflicts)\n",
    "        agents = self.agents.copy()\n",
    "        self.rng.shuffle(agents)\n",
    "        for agent in agents:\n",
    "            self._make_action(agent_id=agent, action_id=actions[agent])\n",
    "        # Get rewards from updated wealth\n",
    "        rewards = {a: self.engine.agents_state[a]['wealth'] - old_w[a]\n",
    "                   for a in self.agents}\n",
    "        # Apply rules to update ecology scores\n",
    "        trash_by_agent = self._update_ecology_scores()\n",
    "        # Terminate or truncate for those neeeded\n",
    "        terminations = self._get_terminations()\n",
    "        truncation = self.num_moves >= self.max_cycles\n",
    "        truncations = {a: truncation for a in self.agents}\n",
    "        # Respawn resources according to respawn probabilities\n",
    "        for a, p in self._get_resource_respawn_probs().items():\n",
    "            u = self.rng.uniform(low=0, high=1)\n",
    "            if u < p:\n",
    "                self.engine.sample_resource(agent_id=a)\n",
    "        # Render next observations\n",
    "        state = self.engine.get_state()\n",
    "        self.current_state = state\n",
    "        # Local perspective state\n",
    "        local_agents_map = self.engine.agents_map(local_mode=True)\n",
    "        local_state = state * np.logical_not(\n",
    "            local_agents_map > 0)\n",
    "        local_state = local_state + local_agents_map\n",
    "        # Render local observations\n",
    "        state_pad = self.engine.pad_state(local_state)\n",
    "        observations = {a: self.engine.local_obs(\n",
    "            state_pad=state_pad, agent_id=a) for a in self.agents}\n",
    "        # Log information\n",
    "        infos = {a: {\n",
    "            'ecology_score': self.ecology_scores[a],\n",
    "            'num_trash': trash_by_agent[a],\n",
    "            'num_resource': self.engine.resource_by_segment(agent_id=a),\n",
    "            'dead_ecology': self.engine.blocked[a]\n",
    "        } for a in self.agents}\n",
    "        # Delete truncated and terminated Agents\n",
    "        if truncation or any(terminations.values()):\n",
    "            self.agents = []\n",
    "        return observations, rewards, terminations, truncations, infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1bc645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: omegaconf in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (2.3.0)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from omegaconf) (6.0.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (from omegaconf) (4.9.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import DictConfig\n",
    "submission_dir = 'submission_vdn'\n",
    "\n",
    "config = DictConfig({\n",
    "    'warmup_steps': 1000,\n",
    "    'eps_start': 0.2,\n",
    "    'eps_decay': 0.996,\n",
    "    'eps_decay_every': 1000,\n",
    "    'acs_dim': 9,\n",
    "    'batch_size': 32,\n",
    "    'update_every': 4,\n",
    "    'buffer_size': 100000,\n",
    "    'initial_batch_episodes': 10,\n",
    "    'learning_rate': 0.0007,\n",
    "    'gamma': 0.99,\n",
    "    'target_updates_freq': 15,\n",
    "    'episodes_per_iter': 2,\n",
    "    'iter_per_save': 20,\n",
    "    'n_iters': 200,\n",
    "    'tau': 0.005,\n",
    "    'output_dir': f'{submission_dir}/agents',\n",
    "    'in_channels': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "540e3da2-d295-4cbf-bf84-958775212576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /Users/asmisnik/Library/Python/3.9/lib/python/site-packages (4.67.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "!pip3 install tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import yaml\n",
    "from math import ceil\n",
    "from omegaconf import DictConfig\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2befe9b-345d-4a4c-9bd6-4793ee92edbe",
   "metadata": {},
   "source": [
    "# Создаем корректную структуру сабмишена\n",
    "\n",
    "\n",
    "Правильная структура директории решения представлена ниже:\n",
    "\n",
    "```\n",
    ".\n",
    "├── utils                          # Директория с модулями, необходимыми для класса агента (опционально)\n",
    "├── model.py                       # Скрипт с реализацией класса агента(ов) и фабричного метода\n",
    "└── agents                         # Директория с артефактами агентов (название фисировано) \n",
    "    ├── agent_0                    # Артефакты отдельного агента (название произвольное)\n",
    "        ├── agent_config.yaml      # Конфиг агента для фабричного метода (название фиксировано)\n",
    "        └── weights_agent_0.pth    # Пример артефакта (веса модели или любые произвольные файлы)\n",
    "    ...                            # Агентов может быть и больше 8\n",
    "    └── agent_n\n",
    "        ├── agent_config.yaml    \n",
    "        └── weights_agent_n.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a5a8339-d291-439e-9cec-31df504e5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_dirs = [\n",
    "    f'{submission_dir}/agents',\n",
    "    f'{submission_dir}/utils'\n",
    "]\n",
    "for d in required_dirs:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bef21-890a-472a-ac81-b67808df4f26",
   "metadata": {},
   "source": [
    "# Вспомогательные функции для Torch\n",
    "\n",
    "Данные вспомогательные функции будут записаны в файл `submission_vdn/utils/utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a13f61-ef4c-420c-982f-ab29a0c93db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission_vdn/utils/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission_vdn/utils/utils.py\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def from_numpy(device, array, dtype=np.float32):\n",
    "    array = array.astype(dtype)\n",
    "    tensor = torch.from_numpy(array)\n",
    "    return tensor.to(device)\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.to('cpu').detach().numpy()\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ca2ea-f354-4257-b9d1-c5b8b2439da1",
   "metadata": {},
   "source": [
    "# Функция сэмплирования из среды\n",
    "\n",
    "Далее реализован параллельный сэмплинг данных из среды `AijMultiagentEnv` при помощи нескольких агентов. В ходе работы функции вызываются следующие методы:\n",
    "\n",
    "1) `reset_state()` - перезагрузка внутреннего состояния агента с началом эпизода\n",
    "2) `get_action()` - получение действия из композитного наблюдения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6135793a-8b3f-4b37-b981-a65ef2a35f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rollouts(\n",
    "    n_rollouts: int,\n",
    "    env: AijMultiagentEnv,\n",
    "    agents: Dict[str, BaseAgent],\n",
    "    verbose: Optional[bool] = False\n",
    ") -> List[List[Dict[str, Any]]]:\n",
    "    rollouts = []\n",
    "    for _ in tqdm(range(n_rollouts), disable=not verbose):\n",
    "        rollout = []\n",
    "        for agent in agents.values():\n",
    "            agent.reset_state()\n",
    "        observations, infos = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = {name: agent.get_action(observation=observations[name])\n",
    "                       for name, agent in agents.items() if name in env.agents}\n",
    "            next_observations, rewards, terminations, truncations, next_infos = env.step(actions)\n",
    "            transition = {\n",
    "                'observations': observations,\n",
    "                'next_observations': next_observations,\n",
    "                'actions': actions,\n",
    "                'rewards': rewards,\n",
    "                'terminations': terminations,\n",
    "                'truncations': truncations\n",
    "            }\n",
    "            observations = next_observations\n",
    "            done = all(truncations.values()) or all(terminations.values())\n",
    "            rollout.append(transition)\n",
    "        rollouts.append(rollout)\n",
    "    return rollouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad3f13a-b8aa-4be0-910b-16dbdbb77b21",
   "metadata": {},
   "source": [
    "# Сэмплирование эпизодов при помощи случайных агентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a95b3385-5b28-4c86-8dff-53db8ccd3584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean agent return for RandomAgent: 6.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_mean_agent_return(batch):\n",
    "    mean_rews = []\n",
    "    for path in batch:\n",
    "        ep_tot_rew = [sum(t['rewards'].values()) for t in path]\n",
    "        ep_tot_rew = sum(ep_tot_rew)\n",
    "        mean_rews.append(ep_tot_rew / 8)\n",
    "    return np.mean(mean_rews)\n",
    "\n",
    "\n",
    "env = AijMultiagentEnv()\n",
    "example_agents = {a: RandomAgent() for a in env.possible_agents}\n",
    "example_batch = sample_rollouts(n_rollouts=10, env=env, agents=example_agents, verbose=True)\n",
    "print(f'Mean agent return for RandomAgent: {get_mean_agent_return(example_batch)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7ea70-ac5f-4149-a62d-e39b6b7c2f46",
   "metadata": {},
   "source": [
    "# Создаем буфер данных\n",
    "\n",
    "Создаем простой буфер данных для хранения эпизодов симуляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f7656bf-27bc-4b54-922c-62a98364e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_transitions: int\n",
    "    ):\n",
    "        self.rollouts = []\n",
    "        self.n_transitions = n_transitions\n",
    "        self.lengths = None\n",
    "\n",
    "    def add_batch(self, rollouts):\n",
    "        self.rollouts.extend(rollouts)\n",
    "        self._evict()\n",
    "        self.lengths = [len(r) for r in self.rollouts]\n",
    "\n",
    "    def _evict(self) -> None:\n",
    "        while len(self) > self.n_transitions:\n",
    "            self.rollouts.pop(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.rollouts) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return sum([len(r) for r in self.rollouts])\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Dict[str, Union[np.ndarray, int]]]:\n",
    "        c_lengths = np.cumsum(self.lengths)\n",
    "        r_ind = np.argwhere(c_lengths > idx).min()\n",
    "        r_ind_last = 0\n",
    "        if r_ind > 0:\n",
    "            r_ind_last = c_lengths[r_ind - 1]\n",
    "        t_ind = idx - r_ind_last\n",
    "        transition = self.rollouts[r_ind][t_ind]\n",
    "        item = {\n",
    "            **transition,\n",
    "            'rollout_index': r_ind,\n",
    "            'transition_index': t_ind\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbe9ad9d-6e45-4ede-b600-f7417bc60cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    collated_data = {k: {} for k in data[0].keys()}\n",
    "    for a in data[0]['observations'].keys():\n",
    "        collated_data['observations'][a] = {}\n",
    "        collated_data['observations'][a]['image'] = np.array(\n",
    "            [d['observations'][a]['image'] for d in data])\n",
    "        collated_data['observations'][a]['proprio'] = np.array(\n",
    "            [d['observations'][a]['proprio'] for d in data])\n",
    "        collated_data['next_observations'][a] = {}\n",
    "        collated_data['next_observations'][a]['image'] = np.array(\n",
    "            [d['next_observations'][a]['image'] for d in data])\n",
    "        collated_data['next_observations'][a]['proprio'] = np.array(\n",
    "            [d['next_observations'][a]['proprio'] for d in data])\n",
    "        collated_data['actions'][a] = np.array([d['actions'][a] for d in data])\n",
    "        collated_data['rewards'][a] = np.array([d['rewards'][a] for d in data])\n",
    "        collated_data['terminations'][a] = np.array([d['terminations'][a] for d in data])\n",
    "        collated_data['truncations'][a] = np.array([d['truncations'][a] for d in data])\n",
    "    collated_data['rollout_index'] = np.array([d['rollout_index'] for d in data])\n",
    "    collated_data['transition_index'] = np.array([d['transition_index'] for d in data])\n",
    "    return collated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba666d-1bb6-41f4-9066-1b0d0c7041be",
   "metadata": {},
   "source": [
    "# Создаем пример dataloader\n",
    "\n",
    "Он понадобится для демонстрации размерностей основных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1263534a-188c-45c9-ba39-7e1cf7b2cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_buffer = ReplayBuffer(100000)\n",
    "example_buffer.add_batch(example_batch)\n",
    "\n",
    "example_dataloader = DataLoader(\n",
    "    dataset=example_buffer,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d76ed4c-03f5-416a-9b2d-91aca776efcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image observation shape: (32, 60, 60, 3)\n",
      "Proprio observation shape: (32, 7)\n",
      "Actions shape: (32,)\n",
      "Rewards shape: (32,)\n",
      "Terminations shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(example_dataloader))\n",
    "\n",
    "print(f\"Image observation shape: {sample['observations']['agent_0']['image'].shape}\")\n",
    "print(f\"Proprio observation shape: {sample['observations']['agent_0']['proprio'].shape}\")\n",
    "print(f\"Actions shape: {sample['actions']['agent_0'].shape}\")\n",
    "print(f\"Rewards shape: {sample['rewards']['agent_0'].shape}\")\n",
    "print(f\"Terminations shape: {sample['terminations']['agent_0'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbff7bd-412f-4283-b2af-d3d3306f0785",
   "metadata": {},
   "source": [
    "# Сетка критик\n",
    "\n",
    "Параграф ниже реализует сеть-критик, которая принимает на вход композитное визуально-проприоцептивное наблюдение и аппроксимирует Q-значения для каждого возможного действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bdee40d-a290-4d47-ac44-d1a3bb65ad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission_vdn/utils/networks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission_vdn/utils/networks.py\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class QCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, acs_dim):\n",
    "        super(QCNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=48, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=48, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(7, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(1600, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, acs_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, image: torch.Tensor, proprio: torch.Tensor) -> torch.Tensor:\n",
    "        # Image stream\n",
    "        bs, h, w, c = image.shape\n",
    "        image = image.permute(0, 3, 1, 2)\n",
    "        image = image / 255.\n",
    "        image_repr = self.cnn(image)\n",
    "        # Proprio stream\n",
    "        proprio_repr = self.mlp(proprio)\n",
    "        # Head\n",
    "        hidden = torch.cat([image_repr, proprio_repr], 1)\n",
    "        q_vals = self.head(hidden)\n",
    "        return q_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b490e60-955d-4f87-a018-57d0f5007a84",
   "metadata": {},
   "source": [
    "# Создаем VDN агента\n",
    "\n",
    "Ниже определим класс VDN агента, стоит отметить. что:\n",
    "\n",
    "Класс агента должен поддерживать следующие методы:\n",
    "- `.load()` - подгрузка агента из директории\n",
    "- `.get_action()` - получение действия из композитного наблюдения\n",
    "- `.reset_state()` - перезагрузка внутреннего состояния агента с началом эпизода\n",
    "\n",
    "Класс агента должен быть отнаследован от абстрактного класса `aij_multiagent_rl.agent.BaseAgent`\n",
    "\n",
    "Файл `model.py` должен содержать фабричный метод агента как показано ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e27ab2c-2b35-43f7-90dc-31a09666578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission_vdn/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission_vdn/model.py\n",
    "import os\n",
    "import abc\n",
    "from copy import deepcopy\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "from torch import nn\n",
    "from utils.networks import QCNN\n",
    "from utils.utils import from_numpy, get_device, to_numpy\n",
    "\n",
    "class BaseAgent(metaclass=abc.ABCMeta):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def load(self, ckpt_dir: str) -> None:\n",
    "        \"\"\"Agent Loading\n",
    "\n",
    "        Loading an agent from the directory with artifacts.\n",
    "\n",
    "        Args:\n",
    "            ckpt_dir: path to an individual directory with artifacts\n",
    "                and the `agent_config.yaml` file for this agent\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_action(self, observation: Dict[str, np.ndarray]) -> int:\n",
    "        \"\"\"Getting action\n",
    "\n",
    "        Getting action from the agent based on visual observation\n",
    "\n",
    "        Args:\n",
    "            observation: dictionary with the following keys and values\n",
    "                \"image\": numpy array with an image of a local field of\n",
    "                    view with dimensions (60, 60, 3) and np.uint8 data type\n",
    "                \"proprio\": numpy array with proprioceptive information\n",
    "                    about the position of the agent on the general map\n",
    "                    and the condition of its inventory with dimensions (7,)\n",
    "                    and np.float32 data type\n",
    "        Returns:\n",
    "            int: index of the selected action\n",
    "                {0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def reset_state(self) -> None:\n",
    "        \"\"\"Resetting the internal state\n",
    "\n",
    "        In case of accumulation of internal context for decision-making\n",
    "        during the episode, this method is called before each new\n",
    "        simulation to clear the internal state before a new episode.\n",
    "        If the internal context is not used, the method can be left in\n",
    "        its current form.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DQNAgent(BaseAgent, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        device,\n",
    "        eval_mode,\n",
    "        warmup_steps=20000,\n",
    "        eps_start=0.2,\n",
    "        eps_decay=0.995,\n",
    "        eps_decay_every=5000,\n",
    "        acs_dim=9,\n",
    "        seed=None\n",
    "    ):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.device = device\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.eval_mode = eval_mode\n",
    "        self.steps_made = 0\n",
    "        self.n_target_updates = 0\n",
    "        self.current_eps = eps_start\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_decay_every = eps_decay_every\n",
    "        self.acs_dim = acs_dim\n",
    "        self.model = model.to(device)\n",
    "        self.target_model = deepcopy(self.model)\n",
    "        self.target_model.to(device)\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0, int(1e6), 1)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def reset_state(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def load(self, ckpt_dir: str) -> None:\n",
    "        self.load_state_dict(\n",
    "            torch.load(\n",
    "                os.path.join(ckpt_dir, \"module.pth\"),\n",
    "                map_location=get_device()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def save(self, ckpt_dir: str) -> None:\n",
    "        torch.save(self.state_dict(), os.path.join(ckpt_dir, 'module.pth'))\n",
    "\n",
    "    def _eps(self):\n",
    "        if self.steps_made < self.warmup_steps:\n",
    "            return 1.\n",
    "        else:\n",
    "            if self.steps_made % self.eps_decay_every == 0:\n",
    "                self.current_eps *= self.eps_decay\n",
    "            return self.current_eps\n",
    "\n",
    "    def _get_action(self, observation: Dict[str, np.ndarray]) -> int:\n",
    "        image = from_numpy(self.device, np.expand_dims(observation['image'], 0))\n",
    "        proprio = from_numpy(self.device, np.expand_dims(observation['proprio'], 0))\n",
    "        qvals = self.model.forward(image=image, proprio=proprio)\n",
    "        qvals = to_numpy(qvals)\n",
    "        return np.argmax(qvals)\n",
    "\n",
    "    def update_target_network(self, tau: float) -> None:\n",
    "        self.n_target_updates += 1\n",
    "        for target_param, param in zip(\n",
    "                self.target_model.parameters(), self.model.parameters()\n",
    "        ):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    def forward(self, image: torch.Tensor, proprio: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model.forward(image, proprio)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward_target(self, image: torch.Tensor, proprio: torch.Tensor) -> torch.Tensor:\n",
    "        return self.target_model.forward(image, proprio)\n",
    "\n",
    "    def get_action(self, observation: Dict[str, np.ndarray]) -> int:\n",
    "        if not self.eval_mode:\n",
    "            self.steps_made += 1\n",
    "        eps = self._eps()\n",
    "        u = self.rng.uniform(0, 1, 1).item()\n",
    "        if u < eps:\n",
    "            acs = self.rng.integers(0, self.acs_dim, 1).item()\n",
    "        else:\n",
    "            acs = self._get_action(observation=observation)\n",
    "        return acs\n",
    "\n",
    "\n",
    "def get_agent(config: DictConfig) -> BaseAgent:\n",
    "    agent = DQNAgent(\n",
    "        device=get_device(),\n",
    "        model=QCNN(in_channels=config.in_channels, acs_dim=config.acs_dim),\n",
    "        eval_mode=config.eval_mode,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        eps_start=config.eps_start,\n",
    "        eps_decay=config.eps_decay,\n",
    "        eps_decay_every=config.eps_decay_every,\n",
    "        acs_dim=config.acs_dim,\n",
    "        seed=config.seed\n",
    "    )\n",
    "    agent.steps_made = config.ckpt_steps_made\n",
    "    agent.current_eps = config.ckpt_eps\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9899b5-09c2-4c28-a586-3b2cd4471ff3",
   "metadata": {},
   "source": [
    "# Импортируем необходимые модули\n",
    "\n",
    "Примечание: как можно видеть ниже, директория с вашими файлами решения будет использована как точка входа для импорта фабричного метода `get_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bc7310a-8cad-4a83-92f4-2da3f1337d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, submission_dir)\n",
    "from model import DQNAgent, get_agent\n",
    "from utils.utils import get_device, from_numpy\n",
    "from utils.networks import QCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9e84b-c086-4ddd-8f40-71b1871804ad",
   "metadata": {},
   "source": [
    "# Определим VDN Trainer\n",
    "\n",
    "Основная логика обновления весов алгоритма VDN реализована в методе `update()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e334a8a-d26b-4820-a08d-ca02d5ac6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDNTrainer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        agents: Dict[str, DQNAgent],\n",
    "        learning_rate: float,\n",
    "        gamma: float = 0.99,\n",
    "        td_criterion=F.smooth_l1_loss,\n",
    "        tau: float = 0.005\n",
    "    ):\n",
    "        super(VDNTrainer, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.td_criterion = td_criterion\n",
    "        self.tau = tau\n",
    "        self.n_updates = 0\n",
    "        self.last_logs = {}\n",
    "\n",
    "        # Set agents\n",
    "        self.agents = nn.ModuleDict(agents)\n",
    "        self.devices = {n: a.device for n, a in self.agents.items()}\n",
    "\n",
    "        # Define optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=(self._get_params()),\n",
    "            lr=self.learning_rate\n",
    "        )\n",
    "\n",
    "    def _get_params(self):\n",
    "        params = []\n",
    "        ids = []\n",
    "        for a in self.agents.values():\n",
    "            model_id = id(a.model)\n",
    "            if model_id not in ids:\n",
    "                params.extend(list(a.model.parameters()))\n",
    "                ids.append(model_id)\n",
    "        return params\n",
    "\n",
    "    def update_target_networks(self) -> None:\n",
    "        for a in self.agents.values():\n",
    "            a.update_target_network(tau=self.tau)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: Dict[str, torch.Tensor],\n",
    "        proprio: Dict[str, torch.Tensor],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        output = {}\n",
    "        for name, agent in self.agents.items():\n",
    "            qvals = agent.forward(images[name], proprio[name])\n",
    "            output[name] = qvals\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward_target(\n",
    "        self,\n",
    "        images: Dict[str, torch.Tensor],\n",
    "        proprio: Dict[str, torch.Tensor],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        output = {}\n",
    "        for name, agent in self.agents.items():\n",
    "            qvals = agent.forward_target(images[name], proprio[name])\n",
    "            output[name] = qvals\n",
    "        return output\n",
    "\n",
    "    def save(self, dir: str, config: dict) -> None:\n",
    "        for name, agent in self.agents.items():\n",
    "            agent_dir = os.path.join(dir, name)\n",
    "            if not os.path.exists(agent_dir):\n",
    "                os.makedirs(agent_dir)\n",
    "            agent.save(ckpt_dir=agent_dir)\n",
    "            with open(os.path.join(agent_dir, 'agent_config.yaml'), 'w') as outfile:\n",
    "                yaml.dump(config, outfile, default_flow_style=False)\n",
    "\n",
    "    def update(self, sample: Dict[str, Any]) -> Dict[str, float]:\n",
    "\n",
    "        # Get device\n",
    "        devs = self.devices\n",
    "\n",
    "        # Unpack data\n",
    "        obs_image = {k: from_numpy(devs[k], v['image']) \n",
    "                     for k, v in sample['observations'].items()}\n",
    "        next_obs_image = {k: from_numpy(devs[k], v['image']) \n",
    "                          for k, v in sample['next_observations'].items()}\n",
    "        obs_proprio = {k: from_numpy(devs[k], v['proprio']) \n",
    "                       for k, v in sample['observations'].items()}\n",
    "        next_obs_proprio = {k: from_numpy(devs[k], v['proprio']) \n",
    "                            for k, v in sample['next_observations'].items()}\n",
    "        actions = {k: from_numpy(devs[k], v) for k, v in sample['actions'].items()}\n",
    "        rewards = {k: from_numpy(devs[k], v) / 10. for k, v in sample['rewards'].items()}\n",
    "        terminations = {k: from_numpy(devs[k], v) for k, v in sample['terminations'].items()}\n",
    "\n",
    "        shared_rewards = torch.cat([r.unsqueeze(-1) for r in rewards.values()], axis=-1)\n",
    "        shared_rewards = shared_rewards.sum(dim=-1, keepdims=True)\n",
    "\n",
    "        # construct target q-values\n",
    "        qa_tp1_target = self.forward_target(next_obs_image, next_obs_proprio)\n",
    "        with torch.no_grad():\n",
    "            qa_tp1_model = self.forward(next_obs_image, next_obs_proprio)\n",
    "\n",
    "        # Select maximum value by agent and sum\n",
    "        q_tp1 = []\n",
    "        for name, qa_tp1_t_a in qa_tp1_target.items():\n",
    "            qa_tp1_m_a = qa_tp1_model[name]\n",
    "            q_tp1_a = torch.gather(qa_tp1_t_a, 1,\n",
    "                                   qa_tp1_m_a.argmax(dim=1, keepdims=True))\n",
    "            term = terminations[name].unsqueeze(1)\n",
    "            q_tp1_a = q_tp1_a * torch.logical_not(term)\n",
    "            q_tp1.append(q_tp1_a)\n",
    "        q_tp1 = torch.cat(q_tp1, axis=-1).sum(dim=-1, keepdims=True)\n",
    "\n",
    "        # Create targets\n",
    "        q_targets = shared_rewards + self.gamma * q_tp1\n",
    "\n",
    "        # Calculate outputs\n",
    "        qa_t = self.forward(obs_image, obs_proprio)\n",
    "\n",
    "        # Select qvalue by action\n",
    "        q_t = []\n",
    "        for name, qa_t_a in qa_t.items():\n",
    "            acs_a = actions[name]\n",
    "            q_t_a = torch.gather(qa_t_a, 1,\n",
    "                                 acs_a.to(torch.long).unsqueeze(1))\n",
    "            q_t.append(q_t_a)\n",
    "        q_t = torch.cat(q_t, axis=-1).sum(dim=-1, keepdims=True)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.td_criterion(q_t, q_targets)\n",
    "\n",
    "        # performing gradient step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.n_updates += 1\n",
    "\n",
    "        return {\n",
    "            'dqn_loss': loss.item(),\n",
    "            'mean_q_value': q_t.mean().item(),\n",
    "            'n_updates': self.n_updates,\n",
    "            'mean_shared_reward': shared_rewards.mean().item(),\n",
    "            'mean_agents_steps': np.mean([a.steps_made for a in self.agents.values()]).item(),\n",
    "            'mean_target_updates': np.mean([a.n_target_updates for a in self.agents.values()]).item(),\n",
    "            'mean_eps': np.mean([a.current_eps for a in self.agents.values()]).item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa823cd-50f8-4a66-a117-4f698db9caf7",
   "metadata": {},
   "source": [
    "# Инициализация симулятора и буфера данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0ce7ff9-0dd9-4a42-8e86-5d2b6207d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AijMultiagentEnv()\n",
    "buffer = ReplayBuffer(config.buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031ec3d-667b-4ca6-b247-5c06915d8a60",
   "metadata": {},
   "source": [
    "# Инициализация VDN trainer\n",
    "\n",
    "Стоит отметить, что все агенты делят веса сетки критика, такая архитектура улучшает сходимость алгоритма и сокращает количество обучаемых параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d19caab-addf-4270-a738-dca76c3255b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "model = QCNN(in_channels=config.in_channels, acs_dim=config.acs_dim)\n",
    "\n",
    "agents = {a: DQNAgent(\n",
    "    device=device,\n",
    "    model=model,\n",
    "    eval_mode=False,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    eps_start=config.eps_start,\n",
    "    eps_decay=config.eps_decay,\n",
    "    eps_decay_every=config.eps_decay_every,\n",
    "    acs_dim=config.acs_dim\n",
    ") for a in env.possible_agents}\n",
    "\n",
    "trainer = VDNTrainer(\n",
    "    agents=agents,\n",
    "    learning_rate=config.learning_rate,\n",
    "    gamma=config.gamma,\n",
    "    tau=config.tau\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e483b6e9-ba20-4362-887a-418620814628",
   "metadata": {},
   "source": [
    "# Заполняем буфер изначальными рандомизированными данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "767cd259-a29f-4d27-8d3d-779a68ee4dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:29<00:00,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Buffer Size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "initial_batch = sample_rollouts(\n",
    "    n_rollouts=config.initial_batch_episodes, env=env, agents=trainer.agents, verbose=True\n",
    ")\n",
    "buffer.add_batch(initial_batch)\n",
    "dataloader = DataLoader(\n",
    "    dataset=buffer,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "print(f'Initial Buffer Size: {len(buffer)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841d878-0204-470b-9045-27fd6e95eb2b",
   "metadata": {},
   "source": [
    "# Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f627ad-6323-4de5-a3e4-305a630a5697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 43/200 [6:54:11<5:31:23, 126.65s/it]   "
     ]
    }
   ],
   "source": [
    "training_logs = []\n",
    "\n",
    "\n",
    "for it in tqdm(range(config.n_iters)):\n",
    "    # print(config.n_iters)\n",
    "    # Sample batch\n",
    "    batch = sample_rollouts(env=env, agents=trainer.agents,\n",
    "                            n_rollouts=config.episodes_per_iter, verbose=False)\n",
    "    batch_size = sum([len(e) for e in batch])\n",
    "\n",
    "    # Add to buffer\n",
    "    buffer.add_batch(rollouts=batch)\n",
    "    data_iter = iter(dataloader)\n",
    "\n",
    "    # Launch update loop\n",
    "    iter_n_updates = max(1, batch_size // config.update_every)\n",
    "    # print(iter_n_updates)\n",
    "    iter_logs = []\n",
    "    \n",
    "    for _ in range(iter_n_updates):\n",
    "        try:\n",
    "            sample = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            sample = next(data_iter)\n",
    "        logs = trainer.update(sample)\n",
    "        if trainer.n_updates % config.target_updates_freq == 0:\n",
    "            trainer.update_target_networks()\n",
    "        iter_logs.append(logs)\n",
    "\n",
    "    # Collect Logs\n",
    "    mean_agent_reward = get_mean_agent_return(batch)\n",
    "    mean_episode_length = batch_size / config.episodes_per_iter\n",
    "    iter_logs = {\n",
    "        'mean_agent_reward': mean_agent_reward,\n",
    "        'mean_episode_length': mean_episode_length,\n",
    "        'batch_size': batch_size,\n",
    "        'iter_n_updates': iter_n_updates,\n",
    "        'buffer_size_transitions': len(buffer),\n",
    "        'buffer_size_episodes': len(buffer.rollouts),\n",
    "        **{k: np.mean([l[k] for l in iter_logs]) for k in iter_logs[0].keys()},\n",
    "    }\n",
    "    training_logs.append(iter_logs)\n",
    "\n",
    "    # Write artifacts\n",
    "    if it > 0 and it % config.iter_per_save == 0:\n",
    "        ckpt_steps_made = min([a.steps_made for a in trainer.agents.values()])\n",
    "        ckpt_eps = min([a.current_eps for a in trainer.agents.values()])\n",
    "        save_config = {\n",
    "            **config,\n",
    "            'eval_mode': True,\n",
    "            'ckpt_steps_made': ckpt_steps_made,\n",
    "            'ckpt_eps': ckpt_eps,\n",
    "            'seed': 42\n",
    "        }\n",
    "        trainer.save(dir=config.output_dir, config=save_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df11a9-a86d-46ae-b704-0ec9b7f82e07",
   "metadata": {},
   "source": [
    "# Визуализация логов обучения\n",
    "\n",
    "Необходимо отметить, что в данном случае средняя награда обучаемых агентов `Mean Agent Reward` не обязательно должна совпадать с целевой метрикой `Mean Focal Score`, вычисляемой тестовой системой. Основная причина заключается в том, что на фазе обучения агенты играют с симметричными агентами, тогда как в тестовой системе симуляции проводятся совместно с различными ботами со скрытыми от Участников стратегиями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7111ca-80db-4554-8485-7590befcabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "n_graphs = len(training_logs[-1].keys())\n",
    "nrows = ceil(n_graphs / ncols)\n",
    "\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(10, 10))\n",
    "for i, key in enumerate(training_logs[-1].keys()):\n",
    "    axs[i // ncols, i % ncols].plot(\n",
    "        [tl[key] for tl in training_logs]\n",
    "    )\n",
    "    axs[i // ncols, i % ncols].set_title(key)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Training Step', ylabel='Value')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
